import json
import re
import time
import html
from typing import List, Dict, Any

import requests
import streamlit as st
import pandas as pd
from openai import OpenAI


# ===============================
# ìœ í‹¸
# ===============================
def strip_b_tags(text: str) -> str:
    """ë„¤ì´ë²„ ê²€ìƒ‰ API ì‘ë‹µì— ì„ì—¬ì˜¤ëŠ” <b> íƒœê·¸ ì œê±°"""
    if not text:
        return ""
    return re.sub(r"</?b>", "", text)


def get_secret(key: str) -> str:
    """Streamlit Cloud Secretsì—ì„œë§Œ ì½ê¸°"""
    return str(st.secrets.get(key, "")).strip()


def naver_local_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 5,
    sort: str = "comment",
) -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIë¡œ ì‹¤ì¡´ ì¥ì†Œ í›„ë³´ ìˆ˜ì§‘
    https://openapi.naver.com/v1/search/local.json
    """
    url = "https://openapi.naver.com/v1/search/local.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    results: List[Dict[str, str]] = []
    for it in data.get("items", []):
        results.append(
            {
                "name": strip_b_tags(it.get("title", "")),
                "address": it.get("roadAddress") or it.get("address") or "",
                "category": it.get("category", ""),
                "tel": it.get("telephone", ""),
                "link": it.get("link", ""),
            }
        )
    return results


def dedupe_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """ì´ë¦„+ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ í›„ë³´ ì¤‘ë³µ ì œê±°"""
    seen = set()
    uniq = []
    for c in candidates:
        key = (c.get("name", "").strip(), c.get("address", "").strip())
        if key in seen:
            continue
        seen.add(key)
        uniq.append(c)
    return uniq


def extract_json_from_text(text: str) -> dict:
    """ëª¨ë¸ì´ JSON ì™¸ í…ìŠ¤íŠ¸ë¥¼ ì„ì—ˆì„ ë•Œ ê°€ì¥ ë°”ê¹¥ JSON ê°ì²´ë¥¼ ì°¾ì•„ íŒŒì‹±"""
    text = (text or "").strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    text = text.replace("```json", "```").replace("```", "")

    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    candidate = text[start : end + 1]
    return json.loads(candidate)


def llm_json(client: OpenAI, system: str, user: str, model: str = "gpt-4.1-mini", retries: int = 2) -> dict:
    """chat.completions ê¸°ë°˜ JSON ì‘ë‹µ ê°•ì œ"""
    for attempt in range(retries + 1):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = resp.choices[0].message.content or ""
        try:
            return extract_json_from_text(text)
        except json.JSONDecodeError:
            if attempt == retries:
                raise
            user = user + "\n\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´."
    raise RuntimeError("Unreachable")


def ensure_three_recommendations(
    recommendations: List[Dict[str, Any]],
    candidates: List[Dict[str, str]],
) -> List[Dict[str, Any]]:
    """
    ì¶”ì²œ ê²°ê³¼ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ candidatesì—ì„œ ë¶€ì¡±ë¶„ì„ ì±„ì›Œ 3ê°œë¡œ ë§ì¶¤
    - ì¤‘ë³µ(ì´ë¦„+ì£¼ì†Œ) ì œê±°
    - rank 1~3 ì¬ì •ë ¬
    """
    def _key(name: str, address: str) -> tuple:
        return (str(name or "").strip(), str(address or "").strip())

    recs = [r for r in recommendations if isinstance(r, dict)]
    recs = sorted(recs, key=lambda x: int(x.get("rank", 999)))

    picked = set()
    cleaned = []
    for r in recs:
        k = _key(r.get("name", ""), r.get("address", ""))
        if k in picked:
            continue
        picked.add(k)
        cleaned.append(r)
    recs = cleaned

    if len(recs) < 3:
        for c in candidates:
            k = _key(c.get("name", ""), c.get("address", ""))
            if k in picked:
                continue
            picked.add(k)
            recs.append({
                "rank": len(recs) + 1,
                "name": c.get("name", ""),
                "reason": "í›„ë³´ ì‹ë‹¹ ì¤‘ ì¡°ê±´ê³¼ ë¬´ë‚œí•˜ê²Œ ì˜ ë§ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤.",
                "address": c.get("address", ""),
                "category": c.get("category", ""),
                "tel": c.get("tel", ""),
                "link": c.get("link", ""),
            })
            if len(recs) == 3:
                break

    recs = recs[:3]
    for i, r in enumerate(recs, start=1):
        r["rank"] = i
    return recs


def make_review_query(name: str, address: str) -> str:
    """
    í›„ê¸° ê²€ìƒ‰ìš© ì¿¼ë¦¬ ìƒì„±:
    - ì‹ë‹¹ëª… + ì£¼ì†Œ ì•ë¶€ë¶„(ì‹œ/êµ¬/ë™ ì •ë„) + 'í›„ê¸°'
    """
    name = (name or "").strip()
    address = (address or "").strip()
    addr_hint = " ".join(address.split()[:3])  # ì˜ˆ: 'ì„œìš¸íŠ¹ë³„ì‹œ ë…¸ì›êµ¬ ë™ì¼ë¡œ...'
    q = f"{name} {addr_hint} í›„ê¸°".strip()
    return re.sub(r"\s+", " ", q)


@st.cache_data(ttl=3600, show_spinner=False)
def naver_blog_search_cached(query: str, client_id: str, client_secret: str, display: int = 3):
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {"query": query, "display": max(1, min(display, 5)), "start": 1, "sort": "sim"}
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        items.append({
            "title": strip_b_tags(html.unescape(it.get("title", ""))),
            "link": it.get("link", ""),
            "desc": strip_b_tags(html.unescape(it.get("description", ""))),
            "thumbnail": it.get("thumbnail", ""),
        })
    return items


@st.cache_data(ttl=3600, show_spinner=False)
def naver_cafe_search_cached(query: str, client_id: str, client_secret: str, display: int = 3):
    url = "https://openapi.naver.com/v1/search/cafearticle.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {"query": query, "display": max(1, min(display, 5)), "start": 1, "sort": "sim"}
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        items.append({
            "title": strip_b_tags(html.unescape(it.get("title", ""))),
            "link": it.get("link", ""),
            "desc": strip_b_tags(html.unescape(it.get("description", ""))),
            "thumbnail": it.get("thumbnail", ""),
        })
    return items


# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="LunchMate ğŸ±", layout="wide")
st.title("ğŸ½ï¸ LunchMate")
st.caption("ì‚¬ìš©ìë‹˜ì˜ ìƒí™©ê³¼ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ëœ ìŒì‹ì  í›„ë³´ ì¤‘ ìµœì ì˜ 3ê³³ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤")

# Secrets ìƒíƒœ í‘œì‹œ
st.sidebar.header("ğŸ” ì—°ê²° ìƒíƒœ")
naver_client_id = get_secret("NAVER_CLIENT_ID")
naver_client_secret = get_secret("NAVER_CLIENT_SECRET")
openai_api_key = get_secret("OPENAI_API_KEY")

st.sidebar.write("ë„¤ì´ë²„ API:", "âœ…" if (naver_client_id and naver_client_secret) else "âŒ (Secrets í•„ìš”)")
st.sidebar.write("OpenAI API:", "âœ…" if openai_api_key else "âŒ (Secrets í•„ìš”)")
st.sidebar.caption("Streamlit Cloud â†’ Settings â†’ Secrets ì— í‚¤ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.")

st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
people = st.sidebar.slider("ì¸ì› ìˆ˜", 1, 10, 5)
distance = st.sidebar.selectbox("ì´ë™ ê±°ë¦¬", ["5ë¶„ ì´ë‚´", "10ë¶„ ì´ë‚´", "ìƒê´€ì—†ìŒ"])
food_type = st.sidebar.multiselect(
    "ìŒì‹ ì¢…ë¥˜",
    ["í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ê¸°íƒ€"],
    default=["í•œì‹"],
)

st.sidebar.header("ğŸ–¼ï¸ í›„ê¸°/ì‚¬ì§„ ì„¤ì •")
show_reviews = st.sidebar.checkbox("í›„ê¸°/ì‚¬ì§„(ë¸”ë¡œê·¸Â·ì¹´í˜) í‘œì‹œ", value=True)
review_display = st.sidebar.slider("ì‹ë‹¹ë‹¹ í›„ê¸° ê°œìˆ˜", 1, 3, 2)

st.subheader("ğŸ“ ì˜¤ëŠ˜ì˜ ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”")
situation = st.text_area(
    "ìì—°ìŠ¤ëŸ½ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”",
    placeholder="ì˜ˆ: ì˜¤ëŠ˜ ì¹œêµ¬ë“¤ê³¼ ì‹ ì´Œì—­ ê·¼ì²˜ì—ì„œ ì ì‹¬ ì‹ì‚¬ë¥¼ í•˜ë ¤ê³  í•´. ë¶„ìœ„ê¸° ì¢‹ì€ ì‹ë‹¹ìœ¼ë¡œ ì¶”ì²œí•´ì¤˜.",
)

col1, col2, col3 = st.columns(3)
with col1:
    if st.button("âš¡ ë¹¨ë¦¬ ë¨¹ê¸°"):
        situation = "ì‹œê°„ì´ ì—†ì–´ì„œ ë¹¨ë¦¬ ë¨¹ì„ ìˆ˜ ìˆëŠ” ê³³ì„ ì°¾ê³  ìˆì–´ìš”"
with col2:
    if st.button("ğŸ‘¥ íŒ€ íšŒì‹"):
        situation = "íŒ€ì¥ë‹˜/íŒ€ì›ë“¤ê³¼ ì¡°ìš©íˆ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì ì‹¬ íšŒì‹ ì¥ì†Œê°€ í•„ìš”í•´ìš”"
with col3:
    if st.button("ğŸ¥£ í•´ì¥ í•„ìš”"):
        situation = "ì–´ì œ ìˆ ì„ ë§ˆì…”ì„œ í•´ì¥ì— ì¢‹ì€ ìŒì‹ì„ ë¨¹ê³  ì‹¶ì–´ìš”"

st.write("")

# ===============================
# ì¶”ì²œ ë²„íŠ¼
# ===============================
if st.button("ğŸ¤– ì ì‹¬ ì¶”ì²œ ë°›ê¸°"):
    if not situation:
        st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not (naver_client_id and naver_client_secret):
        st.error("ë„¤ì´ë²„ Client ID/Secretì´ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secretsì— ë“±ë¡í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not openai_api_key:
        st.error("OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¡œ ë“±ë¡í•´ ì£¼ì„¸ìš”.")
        st.stop()

    client = OpenAI(api_key=openai_api_key)

    # 1) OpenAIë¡œ 'ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ì— ë„£ì„ ê²€ìƒ‰ì–´' ìƒì„±
    system_query = (
        "ë„ˆëŠ” ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIì— ë„£ì„ 'ê²€ìƒ‰ì–´'ë¥¼ ìƒì„±í•˜ëŠ” ë„ìš°ë¯¸ë‹¤.\n"
        "- ì‹ë‹¹ ì´ë¦„ì„ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆë¼.\n"
        "- ê²€ìƒ‰ì— ì˜ ê±¸ë¦´ ì§§ì€ í‚¤ì›Œë“œ ì¡°í•©ë§Œ ë§Œë“¤ì–´ë¼.\n"
        "- ì¶œë ¥ì€ JSONë§Œ. ìŠ¤í‚¤ë§ˆ: { \"queries\": [\"...\", \"...\"] }\n"
        "- queriesëŠ” 2~6ê°œ."
    )
    user_query = (
        f"ìƒí™©: {situation}\n"
        f"ì¸ì›: {people}\n"
        f"ì´ë™ê±°ë¦¬ ì„ í˜¸: {distance}\n"
        f"ì„ í˜¸ ìŒì‹: {', '.join(food_type) if food_type else 'ìƒê´€ì—†ìŒ'}\n\n"
        "ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ì— ë„£ì„ queries 2~6ê°œë¥¼ ë§Œë“¤ì–´ì¤˜."
    )

    with st.spinner("ì¡°ê±´ì„ ë¶„ì„ ì¤‘..."):
        try:
            q_data = llm_json(client, system_query, user_query)
            queries = q_data.get("queries", [])
        except Exception:
            st.error("ê²€ìƒ‰ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    queries = [q.strip() for q in queries if isinstance(q, str) and q.strip()]
    if not queries:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ì…ë ¥ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.")
        st.stop()

    # 2) ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ìœ¼ë¡œ 'ì‹¤ì¡´ í›„ë³´' ìˆ˜ì§‘
    with st.spinner("ì£¼ë³€ ì‹¤ì œ ì‹ë‹¹ í›„ë³´ë¥¼ ì°¾ëŠ” ì¤‘..."):
        candidates: List[Dict[str, str]] = []
        for q in queries[:6]:
            try:
                candidates.extend(
                    naver_local_search(
                        query=q,
                        client_id=naver_client_id,
                        client_secret=naver_client_secret,
                        display=5,
                        sort="comment",
                    )
                )
                time.sleep(0.08)
            except requests.HTTPError as e:
                st.error(f"ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨(HTTP): {e}")
                st.stop()
            except requests.RequestException as e:
                st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                st.stop()

        candidates = dedupe_candidates(candidates)

    if not candidates:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ ì‹ë‹¹ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # 3) í›„ë³´ ì•ˆì—ì„œë§Œ TOP3 ì¶”ì²œ + ì´ìœ  ìƒì„± (í›„ë³´ ë°– ê¸ˆì§€)
    system_rec = (
        "ë„ˆëŠ” ì ì‹¬ ì¶”ì²œ íë ˆì´í„°ë‹¤.\n"
        "- ë°˜ë“œì‹œ candidates ëª©ë¡ì— ìˆëŠ” ì‹ë‹¹ë§Œ ì¶”ì²œí•  ìˆ˜ ìˆë‹¤.\n"
        "- candidatesì— ì—†ëŠ” ì‹ë‹¹ì„ ìƒˆë¡œ ë§Œë“¤ë©´ ì‹¤íŒ¨ë‹¤.\n"
        "- ìˆ«ì(í‰ì /ê°€ê²©/ê±°ë¦¬/ì‹œê°„)ëŠ” ê·¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ì¶”ì²œì€ ìµœëŒ€ 3ê°œ.\n"
        "- ì¶œë ¥ì€ JSONë§Œ. ìŠ¤í‚¤ë§ˆ:\n"
        "{\n"
        "  \"summary\": \"í•œ ì¤„ ê²°ë¡ \",\n"
        "  \"recommendations\": [\n"
        "    {\"rank\": 1, \"name\": \"...\", \"reason\": \"...\", \"address\": \"...\", \"category\": \"...\", \"tel\": \"...\", \"link\": \"...\"}\n"
        "  ]\n"
        "}\n"
    )

    payload = {
        "situation": situation,
        "people": people,
        "distance_pref": distance,
        "food_type": food_type,
        "candidates": candidates[:25],
    }
    user_rec = json.dumps(payload, ensure_ascii=False)

    with st.spinner("í›„ë³´ ì¤‘ì—ì„œ ìµœì ì˜ 3ê³³ì„ ê³ ë¥´ëŠ” ì¤‘..."):
        try:
            r_data = llm_json(client, system_rec, user_rec)
        except Exception:
            st.error("ì¶”ì²œ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    summary = r_data.get("summary", "ì¶”ì²œ ê²°ê³¼ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    recommendations = r_data.get("recommendations", [])

    if not isinstance(recommendations, list) or len(recommendations) == 0:
        st.error("ì¶”ì²œ ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # ì •ë ¬ ë° ìµœëŒ€ 3ê°œë¡œ ê°•ì œ + 3ê°œ ë³´ì •
    recommendations = [r for r in recommendations if isinstance(r, dict)]
    recommendations = sorted(recommendations, key=lambda x: int(x.get("rank", 999)))
    recommendations = recommendations[:3]
    recommendations = ensure_three_recommendations(recommendations, candidates)

    # ===============================
    # ì¶œë ¥ UI (ë” ê¹”ë”í•˜ê²Œ)
    # ===============================
    st.success(f"âœ… **{summary}**")

    st.subheader("ğŸ† ì¶”ì²œ ì‹ë‹¹ TOP 3 (ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜)")

    for r in recommendations:
        with st.container():
            left, right = st.columns([3, 2])

            with left:
                st.markdown(f"### {r.get('rank', '')}ï¸âƒ£ {r.get('name', 'ì´ë¦„ ì—†ìŒ')}")
                st.write(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {r.get('reason', '')}")
                st.write(f"ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {r.get('category', '') or 'ì •ë³´ ì—†ìŒ'}")
                st.write(f"ğŸ“ **ì£¼ì†Œ**: {r.get('address', '') or 'ì •ë³´ ì—†ìŒ'}")

            with right:
                st.write(f"â˜ï¸ **ì „í™”**: {r.get('tel', '') or 'ì •ë³´ ì—†ìŒ'}")
                if r.get("link"):
                    st.link_button("ë„¤ì´ë²„/ì˜ˆì•½ ë§í¬ ì—´ê¸°", r["link"])
                else:
                    st.write("ğŸ”— **ë§í¬**: ì •ë³´ ì—†ìŒ")

            # í›„ê¸°/ì‚¬ì§„(ë¸”ë¡œê·¸Â·ì¹´í˜)
            if show_reviews:
                q = make_review_query(r.get("name", ""), r.get("address", ""))
                with st.expander("ğŸ–¼ï¸ í›„ê¸°/ì‚¬ì§„(ë¸”ë¡œê·¸Â·ì¹´í˜) ë³´ê¸°"):
                    st.caption(f"ê²€ìƒ‰ì–´: {q}")

                    try:
                        blog_posts = naver_blog_search_cached(
                            q, naver_client_id, naver_client_secret, display=review_display
                        )
                        cafe_posts = naver_cafe_search_cached(
                            q, naver_client_id, naver_client_secret, display=review_display
                        )
                    except Exception:
                        blog_posts, cafe_posts = [], []
                        st.write("í›„ê¸° ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

                    if not blog_posts and not cafe_posts:
                        st.write("ê´€ë ¨ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    else:
                        if blog_posts:
                            st.markdown("**ğŸ“ ë¸”ë¡œê·¸ í›„ê¸°**")
                            for p in blog_posts[:review_display]:
                                cols = st.columns([1, 3])
                                with cols[0]:
                                    if p.get("thumbnail"):
                                        st.image(p["thumbnail"], use_container_width=True)
                                with cols[1]:
                                    st.markdown(f"- [{p['title']}]({p['link']})")
                                    if p.get("desc"):
                                        st.caption(p["desc"])

                        if cafe_posts:
                            st.markdown("**ğŸ’¬ ì¹´í˜ í›„ê¸°**")
                            for p in cafe_posts[:review_display]:
                                cols = st.columns([1, 3])
                                with cols[0]:
                                    if p.get("thumbnail"):
                                        st.image(p["thumbnail"], use_container_width=True)
                                with cols[1]:
                                    st.markdown(f"- [{p['title']}]({p['link']})")
                                    if p.get("desc"):
                                        st.caption(p["desc"])

            st.divider()

    st.subheader("ğŸ“‹ ì¶”ì²œ ê²°ê³¼ ìš”ì•½í‘œ")
    df = pd.DataFrame(recommendations)
    cols = [c for c in ["rank", "name", "category", "address", "tel", "link"] if c in df.columns]
    st.dataframe(df[cols], use_container_width=True, hide_index=True)

else:
    st.info("ğŸ‘† ìƒí™©ì„ ì…ë ¥í•˜ê³  **ì ì‹¬ ì¶”ì²œ ë°›ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
