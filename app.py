import json
import re
import time
import html
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import quote_plus

import requests
import streamlit as st
import pandas as pd
from openai import OpenAI


# ===============================
# ê¸°ë³¸ ë””í´íŠ¸ ì„¤ì • (ì´ˆê¸°ê°’)
# ===============================
# âœ… ì¸ì›ìˆ˜ ë””í´íŠ¸: 'ìƒê´€ì—†ìŒ'
PEOPLE_OPTIONS = ["ìƒê´€ì—†ìŒ"] + [f"{i}ëª…" for i in range(1, 11)]
DEFAULT_PEOPLE_INDEX = 0

DISTANCE_OPTIONS = ["5ë¶„ ì´ë‚´", "10ë¶„ ì´ë‚´", "ìƒê´€ì—†ìŒ"]
DEFAULT_DISTANCE_INDEX = 2  # "ìƒê´€ì—†ìŒ"

FOOD_OPTIONS = [
    "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ê¸°íƒ€",
    "ì¹´í˜", "ë””ì €íŠ¸"
]
DEFAULT_FOOD_TYPES: List[str] = []

TOP_K = 5

# í›„ë³´ í™•ì¥ / ì„±ëŠ¥
MAX_QUERIES = 6
LOCAL_DISPLAY_PER_QUERY = 5
CANDIDATE_LIMIT_FOR_LLM = 40
REQUEST_SLEEP_SEC = 0.08

# ë¸”ë¡œê·¸ ë¶„ì„ìš©(ìŠ¤ë‹ˆí«ë§Œ ì‚¬ìš©)
BLOG_PER_PLACE_FOR_SCORING = 3

# ìŠ¤ì½”ì–´ë§ ì´í›„ LLMì— ë„˜ê¸¸ í›„ë³´ ìˆ˜
LLM_RERANK_POOL = 25

# âœ… ê²°ê³¼ ì¹´ë“œ ì´ë¯¸ì§€(ì´ë¯¸ì§€ ê²€ìƒ‰ API)
IMAGE_PER_PLACE = 1  # 1~5 (ë„¤ì´ë²„ ì´ë¯¸ì§€ API display ì œí•œì— ë§ì¶° 5 ì´í•˜ë¡œ ìœ ì§€)


# ===============================
# ìœ í‹¸
# ===============================
def strip_b_tags(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"</?b>", "", text)


def get_secret(key: str) -> str:
    return str(st.secrets.get(key, "")).strip()


def safe_int(x: Any, default: int = 999) -> int:
    try:
        return int(x)
    except Exception:
        return default


def normalize_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def split_csv(text: str) -> List[str]:
    parts = [p.strip() for p in (text or "").split(",")]
    return [p for p in parts if p]


def any_kw(blob: str, kws: List[str]) -> bool:
    b = normalize_text(blob)
    return any(normalize_text(k) in b for k in kws if k)


def count_kw_hits(blob: str, kws: List[str]) -> int:
    b = normalize_text(blob)
    hits = 0
    for k in kws:
        kk = normalize_text(k)
        if kk and kk in b:
            hits += 1
    return hits


def parse_people_value(choice: str) -> int:
    """'ìƒê´€ì—†ìŒ' -> 0, 'Nëª…' -> N"""
    choice = (choice or "").strip()
    if choice == "ìƒê´€ì—†ìŒ":
        return 0
    m = re.search(r"(\d+)", choice)
    return int(m.group(1)) if m else 0


def naver_local_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 5,
    sort: str = "comment",
    start: int = 1,
) -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIë¡œ ì‹¤ì¡´ ì¥ì†Œ í›„ë³´ ìˆ˜ì§‘
    https://openapi.naver.com/v1/search/local.json
    """
    url = "https://openapi.naver.com/v1/search/local.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": max(1, start),
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    results: List[Dict[str, str]] = []
    for it in data.get("items", []):
        results.append(
            {
                "name": strip_b_tags(it.get("title", "")),
                "address": it.get("roadAddress") or it.get("address") or "",
                "category": it.get("category", ""),
                "link": it.get("link", ""),
            }
        )
    return results


def dedupe_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    uniq = []
    for c in candidates:
        key = ((c.get("name", "") or "").strip(), (c.get("address", "") or "").strip())
        if key in seen:
            continue
        seen.add(key)
        uniq.append(c)
    return uniq


def filter_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ìŒì‹ì /ì¹´í˜ ì¶”ì²œ ì„œë¹„ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ, ëª…ë°±íˆ ë¹„ì‹ìŒë£Œ ì—…ì¢…ë§Œ ì œê±°
    """
    bad_keywords = [
        "í•™ì›", "ê³µì¸ì¤‘ê°œ", "ë¶€ë™ì‚°", "ë¯¸ìš©", "ë„¤ì¼", "í”¼ë¶€", "ì„±í˜•",
        "í—¬ìŠ¤", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ì„¸íƒ", "ìˆ˜ë¦¬", "ì •ë¹„", "ë Œíƒˆ",
        "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ì•½êµ­", "ë³‘ì›", "ì˜ì›", "ì¹˜ê³¼", "í•œì˜ì›",
        "ì£¼ìœ ", "ìë™ì°¨", "ì¸í…Œë¦¬ì–´", "ê°€êµ¬", "ë§ˆíŠ¸",
    ]
    out = []
    for c in candidates:
        name = (c.get("name") or "").strip()
        cat = (c.get("category") or "").strip()
        blob = f"{name} {cat}"
        if any(k in blob for k in bad_keywords):
            continue
        out.append(c)
    return out


def extract_json_from_text(text: str) -> dict:
    text = (text or "").strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    text = text.replace("```json", "```").replace("```", "")

    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    candidate = text[start: end + 1]
    return json.loads(candidate)


def llm_json(
    client: OpenAI,
    system: str,
    user: str,
    model: str = "gpt-4.1-mini",
    retries: int = 2
) -> dict:
    for attempt in range(retries + 1):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = resp.choices[0].message.content or ""
        try:
            return extract_json_from_text(text)
        except json.JSONDecodeError:
            if attempt == retries:
                raise
            user = user + "\n\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´."
    raise RuntimeError("Unreachable")


def ensure_k_recommendations(
    recommendations: List[Dict[str, Any]],
    candidates: List[Dict[str, Any]],
    k: int,
) -> List[Dict[str, Any]]:
    """
    ì¶”ì²œ ê²°ê³¼ê°€ kê°œ ë¯¸ë§Œì´ë©´ candidatesì—ì„œ ë¶€ì¡±ë¶„ì„ ì±„ì›Œ kê°œë¡œ ë§ì¶¤
    - ì¤‘ë³µ(ì´ë¦„+ì£¼ì†Œ) ì œê±°
    - rank 1~k ì¬ì •ë ¬
    """
    def _key(name: str, address: str) -> tuple:
        return (str(name or "").strip(), str(address or "").strip())

    recs = [r for r in recommendations if isinstance(r, dict)]
    recs = sorted(recs, key=lambda x: safe_int(x.get("rank", 999)))

    picked = set()
    cleaned = []
    for r in recs:
        k0 = _key(r.get("name", ""), r.get("address", ""))
        if k0 in picked:
            continue
        picked.add(k0)
        cleaned.append(r)
    recs = cleaned

    if len(recs) < k:
        for c in candidates:
            k0 = _key(c.get("name", ""), c.get("address", ""))
            if k0 in picked:
                continue
            picked.add(k0)
            recs.append({
                "rank": len(recs) + 1,
                "name": c.get("name", ""),
                "reason": "í›„ë³´ ì¤‘ ì¡°ê±´ê³¼ ë¬´ë‚œí•˜ê²Œ ì˜ ë§ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤.",
                "tags": ["í›„ë³´ê¸°ë°˜"],
                "address": c.get("address", ""),
                "category": c.get("category", ""),
                "link": c.get("link", ""),
            })
            if len(recs) == k:
                break

    recs = recs[:k]
    for i, r in enumerate(recs, start=1):
        r["rank"] = i
    return recs


def make_review_query(name: str, address: str) -> str:
    name = (name or "").strip()
    address = (address or "").strip()
    addr_hint = " ".join(address.split()[:3])
    q = f"{name} {addr_hint} í›„ê¸°".strip()
    return re.sub(r"\s+", " ", q)


@st.cache_data(ttl=3600, show_spinner=False)
def naver_blog_search_cached(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 3,
    sort: str = "sim",
):
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        items.append({
            "title": strip_b_tags(html.unescape(it.get("title", ""))),
            "link": it.get("link", ""),
            "desc": strip_b_tags(html.unescape(it.get("description", ""))),
        })
    return items


@st.cache_data(ttl=3600, show_spinner=False)
def naver_image_search_cached(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 1,
    sort: str = "sim",
):
    """
    ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ API
    https://openapi.naver.com/v1/search/image
    - items[].thumbnail : ì„¬ë„¤ì¼ URL
    - items[].link      : ì›ë³¸ ì´ë¯¸ì§€ URL
    """
    url = "https://openapi.naver.com/v1/search/image"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,        # sim/date
        "filter": "all",     # all/large/medium/small
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()
    return data.get("items", [])


def naver_map_search_url(place_name: str, address: str = "") -> str:
    q = (place_name or "").strip()
    if address:
        q = f"{q} {address.split()[0]}".strip()
    return f"https://map.naver.com/v5/search/{quote_plus(q)}"


def build_cache_key(payload: Dict[str, Any]) -> str:
    compact = {
        "start": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance": payload.get("distance_pref", ""),
        "food": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "visit_type": payload.get("visit_type", ""),
        "blog_sort": payload.get("blog_sort", "sim"),
        "quick_tags": payload.get("quick_tags", []),
    }
    return json.dumps(compact, ensure_ascii=False, sort_keys=True)


# ===============================
# â€œëª…í™•í•œ ê¸°ì¤€â€ ë£°ì…‹ (í˜¼ë°¥ ê°•í™”)
# ===============================
SOLO_HARD_EXCLUDE = ["ì›¨ë”©", "ëŒ€ê´€", "ì—°íšŒ", "í–‰ì‚¬", "ì„¸ë¯¸ë‚˜", "ë·”í˜", "ëŒì”ì¹˜"]
SOLO_STRONG_PENALTY = [
    "ë‹¨ì²´", "ë‹¨ì²´ì„", "ë‹¨ì²´ê°€ëŠ¥", "íšŒì‹", "ëª¨ì„", "ë£¸", "ë£¸ì™„ë¹„", "ê°€ì¡±ëª¨ì„",
    "ì˜ˆì•½í•„ìˆ˜", "ëŒ€í˜•", "ì—°ë§",
]
SOLO_POSITIVE = ["í˜¼ë°¥", "í˜¼ì", "1ì¸", "1ì¸ì‹ì‚¬", "ë°”ìë¦¬", "ì¹´ìš´í„°", "í‚¤ì˜¤ìŠ¤í¬", "í¬ì¥", "í…Œì´í¬ì•„ì›ƒ"]
SOLO_CATEGORY_PENALTY = ["ê³ ê¸°", "ì‚¼ê²¹", "ê°ˆë¹„", "í•œìš°", "ë¬´í•œë¦¬í•„", "ë°”ë² í", "ê³±ì°½", "ë§‰ì°½", "ì°¸ì¹˜", "íšŸì§‘", "ëŒ€ê²Œ", "ì½”ìŠ¤", "ë·”í˜"]

BUDGET_POSITIVE = ["ê°€ì„±ë¹„", "ì €ë ´", "ì°©í•œê°€ê²©", "ë§Œì›", "ë§Œì›ëŒ€", "ì ì‹¬íŠ¹ì„ ", "ì„¸íŠ¸", "ë°±ë°˜"]
BUDGET_NEGATIVE = ["ì˜¤ë§ˆì¹´ì„¸", "íŒŒì¸ë‹¤ì´ë‹", "ì½”ìŠ¤", "í”„ë¦¬ë¯¸ì—„", "ê³ ê¸‰", "ë¹„ì‹¼", "ê³ ê°€"]


def infer_intents(payload: Dict[str, Any]) -> Dict[str, bool]:
    situation = payload.get("situation", "")
    prefer = payload.get("prefer", "")
    people = payload.get("people", 0)

    blob = f"{situation} {prefer}".strip()
    solo = (people == 1) or any_kw(blob, ["í˜¼ë°¥", "í˜¼ì", "1ì¸", "1ì¸ì‹ì‚¬", "í˜¼ìˆ "])
    budget = any_kw(blob, ["ê°€ì„±ë¹„", "ì €ë ´", "ì‹¸ê²Œ", "ì°©í•œê°€ê²©", "ë§Œì›", "ë§Œì›ëŒ€"])
    vegan = any_kw(blob, ["ë¹„ê±´", "vegan", "ì±„ì‹", "ë½í† ", "ì˜¤ë³´"])
    diet = any_kw(blob, ["ë‹¤ì´ì–´íŠ¸", "ì €íƒ„", "í‚¤í† ", "ìƒëŸ¬ë“œ", "ë‹¨ë°±ì§ˆ"])
    return {"solo": solo, "budget": budget, "vegan": vegan, "diet": diet}


def candidate_signal_blob(candidate: Dict[str, str], blog_snippets: List[str]) -> str:
    name = candidate.get("name", "") or ""
    category = candidate.get("category", "") or ""
    addr = candidate.get("address", "") or ""
    blog = " ".join(blog_snippets[:10])
    return f"{name} {category} {addr} {blog}".strip()


def score_candidate_for_payload(
    payload: Dict[str, Any],
    candidate: Dict[str, str],
    blog_snippets: List[str],
    intents: Dict[str, bool],
) -> Tuple[int, Dict[str, Any]]:
    score = 0
    reasons = []

    blob = candidate_signal_blob(candidate, blog_snippets)
    name_cat = f"{candidate.get('name','')} {candidate.get('category','')}".strip()

    exclude_terms = split_csv(payload.get("exclude", ""))
    if exclude_terms and any_kw(blob, exclude_terms):
        score -= 120
        reasons.append(f"ì œì™¸ì¡°ê±´ ë§¤ì¹­(-120): {', '.join(exclude_terms)}")

    if intents.get("solo"):
        if any_kw(blob, SOLO_HARD_EXCLUDE):
            score -= 999
            reasons.append("í˜¼ë°¥: í•˜ë“œ ì œì™¸ ìš©ë„/ì—…ì¢…(-999)")
        hits = count_kw_hits(blob, SOLO_STRONG_PENALTY)
        if hits:
            penalty = min(80 * hits, 240)
            score -= penalty
            reasons.append(f"í˜¼ë°¥: ë‹¨ì²´/ëª¨ì„ ì‹œê·¸ë„({hits}) ê°ì (-{penalty})")

        pos_hits = count_kw_hits(blob, SOLO_POSITIVE)
        if pos_hits:
            bonus = min(50 * pos_hits, 150)
            score += bonus
            reasons.append(f"í˜¼ë°¥: 1ì¸ ì¹œí™” ì‹œê·¸ë„({pos_hits}) ê°€ì (+{bonus})")

        cat_hits = count_kw_hits(name_cat, SOLO_CATEGORY_PENALTY)
        if cat_hits:
            penalty = min(70 * cat_hits, 210)
            score -= penalty
            reasons.append(f"í˜¼ë°¥: ì—…ì¢… íŒ¨ë„í‹°({cat_hits})(-{penalty})")

    if intents.get("budget"):
        pos = count_kw_hits(blob, BUDGET_POSITIVE)
        if pos:
            bonus = min(35 * pos, 140)
            score += bonus
            reasons.append(f"ê°€ì„±ë¹„: ê¸ì • ì‹œê·¸ë„({pos})(+{bonus})")
        neg = count_kw_hits(blob, BUDGET_NEGATIVE)
        if neg:
            penalty = min(60 * neg, 180)
            score -= penalty
            reasons.append(f"ê°€ì„±ë¹„: ê³ ê°€ ì‹œê·¸ë„({neg})(-{penalty})")

    # âœ… ë¹ ë¥¸ íƒœê·¸ ë°˜ì˜(ì•½~ì¤‘ê°„)
    quick_tags = payload.get("quick_tags", []) or []
    if quick_tags:
        if any_kw(blob, quick_tags):
            score += 40
            reasons.append("ë¹ ë¥¸ íƒœê·¸ ë§¤ì¹­(+40)")

    food_types = payload.get("food_type") or []
    if food_types:
        if any_kw(candidate.get("category", ""), food_types) or any_kw(candidate.get("name", ""), food_types):
            score += 35
            reasons.append("ì„ íƒí•œ ìŒì‹/ì¹´í˜ ì¢…ë¥˜ ë§¤ì¹­(+35)")

    visit = payload.get("visit_type", "ìƒê´€ì—†ìŒ")
    if visit == "ì¹´í˜/ë””ì €íŠ¸":
        if any_kw(blob, ["ì¹´í˜", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ì»¤í”¼"]):
            score += 40
            reasons.append("ë°©ë¬¸ëª©ì (ì¹´í˜/ë””ì €íŠ¸) ë§¤ì¹­(+40)")
        else:
            score -= 20
            reasons.append("ë°©ë¬¸ëª©ì (ì¹´í˜/ë””ì €íŠ¸) ë¶ˆì¼ì¹˜(-20)")
    elif visit in ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…"]:
        if any_kw(blob, ["ë¸ŒëŸ°ì¹˜", "ì•„ì¹¨"]):
            score += 15 if visit == "ì•„ì¹¨" else 0
        if any_kw(blob, ["ì ì‹¬íŠ¹ì„ ", "ëŸ°ì¹˜"]):
            score += 15 if visit == "ì ì‹¬" else 0
        if any_kw(blob, ["ìˆ ", "ì•ˆì£¼", "í˜¸í”„", "í¬ì°¨"]):
            score += 15 if visit == "ì €ë…" else 0

    score += 10  # ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜ ì‹ ë¢° ê°€ì 

    meta = {"score": score, "score_notes": reasons[:8]}
    return score, meta


def solo_gate_filter(sorted_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    kept = []
    dropped = []
    penalty_set = [normalize_text(x) for x in SOLO_STRONG_PENALTY]
    for c in sorted_candidates:
        blob = normalize_text(c.get("_signal_blob", ""))
        if any(k in blob for k in penalty_set):
            dropped.append(c)
        else:
            kept.append(c)

    if len(kept) >= 15:
        return kept

    dropped = sorted(dropped, key=lambda x: safe_int(x.get("_score", -999999)), reverse=True)
    return kept + dropped[: max(0, 15 - len(kept))]


# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="LunchMate ğŸ±", layout="wide")

# âœ… ìŠ¤í¬ë¡¤ ì ê¹€ ë°©ì§€ CSS
st.markdown(
    """
    <style>
    html, body { overflow: auto !important; height: auto !important; }
    [data-testid="stAppViewContainer"] { overflow: auto !important; }
    [data-testid="stMain"] { overflow: auto !important; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ½ï¸ LunchMate ğŸ½ï¸")
st.caption(f"ì‚¬ìš©ìë‹˜ì˜ ìƒí™©ê³¼ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì‹ì /ì¹´í˜ í›„ë³´ ì¤‘ ìµœì ì˜ {TOP_K}ê³³ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.")

naver_client_id = get_secret("NAVER_CLIENT_ID")
naver_client_secret = get_secret("NAVER_CLIENT_SECRET")
openai_api_key = get_secret("OPENAI_API_KEY")

if "candidate_cache_key" not in st.session_state:
    st.session_state["candidate_cache_key"] = None
if "candidates" not in st.session_state:
    st.session_state["candidates"] = []
# âœ… ë©”ì¸ì— ìœ„ì¹˜í•œ ë¹ ë¥¸ íƒœê·¸ ìƒíƒœ ìœ ì§€
if "quick_tags_main" not in st.session_state:
    st.session_state["quick_tags_main"] = []


# ===============================
# ì‚¬ì´ë“œë°” (âœ… ë¹ ë¥¸ íƒœê·¸ ì œê±°!)
# ===============================
st.sidebar.header("ğŸ•’ ë§¤ì¥ ë°©ë¬¸ ëª©ì ")
visit_type = st.sidebar.selectbox(
    "ì¶”ì²œ ë°›ì„ ì¢…ë¥˜",
    ["ìƒê´€ì—†ìŒ", "ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì¹´í˜/ë””ì €íŠ¸"],
    index=0
)

st.sidebar.header("ğŸ“ ì¶œë°œ ìœ„ì¹˜(ì •í™•ë„ ê°œì„ )")
start_location = st.sidebar.text_input("ì¶œë°œì§€(íšŒì‚¬/ì—­/ì£¼ì†Œ)", placeholder="ì˜ˆ: ì‹ ì´Œì—­, ê°•ë‚¨ì—­, íŒêµì—­")

st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
people_choice = st.sidebar.selectbox("ì¸ì› ìˆ˜", PEOPLE_OPTIONS, index=DEFAULT_PEOPLE_INDEX)
people = parse_people_value(people_choice)

distance = st.sidebar.selectbox("ì´ë™ ê±°ë¦¬", DISTANCE_OPTIONS, index=DEFAULT_DISTANCE_INDEX)
food_type = st.sidebar.multiselect("ìŒì‹/ì¹´í˜ ì¢…ë¥˜", FOOD_OPTIONS, default=DEFAULT_FOOD_TYPES)

st.sidebar.header("ğŸš« ì œì™¸ / âœ… ì„ í˜¸")
exclude_text = st.sidebar.text_input("ì œì™¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ë§¤ìš´ ìŒì‹, íšŒ, ì›¨ì´íŒ…")
prefer_text = st.sidebar.text_input("ì„ í˜¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ì¡°ìš©í•œ ê³³, ê°€ì„±ë¹„, ë””ì €íŠ¸")

st.sidebar.header("ğŸ–¼ï¸ í›„ê¸°/ì‚¬ì§„ ì„¤ì •")
show_reviews = st.sidebar.checkbox("ë¸”ë¡œê·¸ í›„ê¸° í‘œì‹œ", value=True)
review_display = st.sidebar.slider("ì¥ì†Œë‹¹ ë¸”ë¡œê·¸ í›„ê¸° ê°œìˆ˜", 1, 3, 2)
blog_sort = st.sidebar.radio("í›„ê¸° ì •ë ¬", ["ì—°ê´€ë„(ì¶”ì²œ)", "ìµœì‹ ìˆœ"], index=0)
blog_sort_param = "sim" if blog_sort.startswith("ì—°ê´€ë„") else "date"

st.sidebar.divider()
debug_mode = st.sidebar.checkbox("ğŸ§ª ë””ë²„ê·¸(í›„ë³´ ì ìˆ˜/í•„í„° ë³´ê¸°)", value=False)


# ===============================
# ë©”ì¸ ì…ë ¥ (âœ… ë¹ ë¥¸ íƒœê·¸ë¥¼ í”„ë¡¬í”„íŠ¸ ë°”ë¡œ ì•„ë˜ë¡œ ì´ë™)
# ===============================
st.subheader("ğŸ“ í¬ë§ ì¡°ê±´ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”")
situation = st.text_area(
    "ììœ ë¡­ê²Œ ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”(ì·¨í–¥, ë°©ë¬¸ ì§€ì—­, ì¸ì› ìˆ˜, ì‹ì‚¬ ìƒí™© ë“±)",
    placeholder="ì˜ˆ: ì‹ ì´Œì—­ì—ì„œ ì¹œêµ¬ì™€ ì ì‹¬ ë¨¹ì„ê±°ì•¼. ê°€ì„±ë¹„ ì¢‹ì€ ì¤‘ì‹ ìŒì‹ì  ì¶”ì²œí•´ì¤˜. / ì ì‹¤ì—ì„œ ì¹´ê³µí•˜ê¸° ì¢‹ì€ ì¹´í˜ ì°¾ì•„ì¤˜.",
)

# âœ… í”„ë¡¬í”„íŠ¸ ì…ë ¥ë€ ë°”ë¡œ ì•„ë˜ì— 'ë¹ ë¥¸ íƒœê·¸' ë°°ì¹˜
st.markdown("### ğŸ§© ë¹ ë¥¸ íƒœê·¸(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)")
QUICK_TAGS = [
    "í˜¼ë°¥", "ì¡°ìš©í•œ", "ê°€ì„±ë¹„", "ì›¨ì´íŒ… ì ì€", "ë§¤ìš´ ìŒì‹",
    "ë°ì´íŠ¸", "ë‹¨ì²´ ê°€ëŠ¥", "í¬ì¥/í…Œì´í¬ì•„ì›ƒ",
    "ë‹¤ì´ì–´íŠ¸", "ë¹„ê±´", "ìƒëŸ¬ë“œ", "ë””ì €íŠ¸", "ë¸ŒëŸ°ì¹˜",
    "ì•¼ì‹", "ìˆ /ì•ˆì£¼", "ì¹´ê³µ",
]
quick_tags = st.multiselect(
    "ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”",
    QUICK_TAGS,
    default=st.session_state.get("quick_tags_main", []),
    key="quick_tags_main",
)

# âœ… ì ìš© í‘œì‹œ(ì‚¬ìš©ì ê°€ì‹œì„±)
if quick_tags:
    st.success(f"âœ… ë¹ ë¥¸ íƒœê·¸ ì ìš©ë¨: {', '.join(quick_tags)}")
else:
    st.caption("ì„ íƒí•œ ë¹ ë¥¸ íƒœê·¸ê°€ ì—†ì–´ìš”. í•„ìš”í•˜ë©´ ìœ„ì—ì„œ ê³¨ë¼ì£¼ì„¸ìš”.")

st.write("")


def require_secrets_or_stop():
    if not (naver_client_id and naver_client_secret and openai_api_key):
        st.error("ì„œë¹„ìŠ¤ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()


def generate_queries(client: OpenAI, payload: Dict[str, Any]) -> List[str]:
    system_query = (
        "ë„ˆëŠ” ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIì— ë„£ì„ 'ê²€ìƒ‰ì–´(queries)'ë¥¼ ìƒì„±í•˜ëŠ” ë„ìš°ë¯¸ë‹¤.\n"
        "- ì¥ì†Œ ì´ë¦„ì„ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ë°˜ë“œì‹œ ì‚¬ìš©ìì˜ ì¶œë°œì§€(ì§€ì—­/ì—­/ì£¼ì†Œ) ì •ë³´ë¥¼ queriesì— ë°˜ì˜í•˜ë¼.\n"
        "- ì¶”ì²œ ë°›ì„ ì¢…ë¥˜(ì•„ì¹¨/ì ì‹¬/ì €ë…/ì¹´í˜) ì •ë³´ë¥¼ ë°˜ì˜í•˜ë¼.\n"
        "- ê²€ìƒ‰ì— ì˜ ê±¸ë¦¬ëŠ” ì§§ì€ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ.\n"
        "- ì¶œë ¥ì€ JSONë§Œ. ìŠ¤í‚¤ë§ˆ: { \"queries\": [\"...\", \"...\"] }\n"
        "- queriesëŠ” 3~6ê°œ."
    )

    food = payload.get("food_type") or []
    food_str = ", ".join(food) if food else "(ì„ íƒ ì—†ìŒ)"

    visit = payload.get("visit_type", "ìƒê´€ì—†ìŒ")
    visit_hint = {
        "ì•„ì¹¨": "ì•„ì¹¨/ë¸ŒëŸ°ì¹˜",
        "ì ì‹¬": "ì ì‹¬",
        "ì €ë…": "ì €ë…/ìˆ ìë¦¬",
        "ì¹´í˜/ë””ì €íŠ¸": "ì¹´í˜/ë””ì €íŠ¸",
        "ìƒê´€ì—†ìŒ": "ë§›ì§‘/ì¹´í˜",
    }.get(visit, "ë§›ì§‘/ì¹´í˜")

    qt = payload.get("quick_tags") or []
    qt_str = ", ".join(qt) if qt else "(ì—†ìŒ)"

    user_query = (
        f"ì¶”ì²œ ì¢…ë¥˜: {visit} ({visit_hint})\n"
        f"ì¶œë°œì§€: {payload.get('start_location') or '(ë¯¸ì…ë ¥)'}\n"
        f"ìƒí™©: {payload.get('situation')}\n"
        f"ì¸ì›: {payload.get('people') or 'ìƒê´€ì—†ìŒ'}\n"
        f"ì´ë™ê±°ë¦¬ ì„ í˜¸: {payload.get('distance_pref')}\n"
        f"ì„ í˜¸ ìŒì‹/ì¹´í˜ ì¢…ë¥˜: {food_str}\n"
        f"ë¹ ë¥¸ íƒœê·¸: {qt_str}\n"
        f"ì œì™¸ ì¡°ê±´: {payload.get('exclude') or '(ì—†ìŒ)'}\n"
        f"ì„ í˜¸ ì¡°ê±´: {payload.get('prefer') or '(ì—†ìŒ)'}\n\n"
        "ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ì— ë„£ì„ queries 3~6ê°œë¥¼ ë§Œë“¤ì–´ì¤˜."
    )

    q_data = llm_json(client, system_query, user_query)
    queries = q_data.get("queries", [])
    queries = [q.strip() for q in queries if isinstance(q, str) and q.strip()]

    start = (payload.get("start_location") or "").strip()
    if start:
        patched = []
        for q in queries:
            patched.append(q if start in q else f"{start} {q}".strip())
        queries = patched

    uniq = []
    seen = set()
    for q in queries:
        if q in seen:
            continue
        seen.add(q)
        uniq.append(q)
    return uniq[:MAX_QUERIES]


def collect_candidates(queries: List[str]) -> List[Dict[str, str]]:
    candidates: List[Dict[str, str]] = []
    for q in queries[:MAX_QUERIES]:
        candidates.extend(
            naver_local_search(
                query=q,
                client_id=naver_client_id,
                client_secret=naver_client_secret,
                display=LOCAL_DISPLAY_PER_QUERY,
                sort="comment",
                start=1,
            )
        )
        time.sleep(REQUEST_SLEEP_SEC)

    candidates = dedupe_candidates(candidates)
    candidates = filter_candidates(candidates)
    return candidates


def score_and_prepare_candidates(
    payload: Dict[str, Any],
    candidates: List[Dict[str, str]],
    blog_sort_param: str,
) -> List[Dict[str, Any]]:
    intents = infer_intents(payload)

    enriched: List[Dict[str, Any]] = []
    for c in candidates:
        name = c.get("name", "")
        addr = c.get("address", "")
        q = make_review_query(name, addr)

        try:
            posts = naver_blog_search_cached(
                q, naver_client_id, naver_client_secret,
                display=BLOG_PER_PLACE_FOR_SCORING,
                sort=blog_sort_param,
            )
            snippets = []
            for p in posts[:BLOG_PER_PLACE_FOR_SCORING]:
                snippets.append(f"{p.get('title','')} {p.get('desc','')}".strip())
        except Exception:
            snippets = []

        score, meta = score_candidate_for_payload(payload, c, snippets, intents)
        signal_blob = candidate_signal_blob(c, snippets)

        c2 = dict(c)
        c2["_score"] = score
        c2["_score_notes"] = meta.get("score_notes", [])
        c2["_signal_blob"] = signal_blob
        enriched.append(c2)

    enriched = sorted(enriched, key=lambda x: safe_int(x.get("_score", -999999)), reverse=True)

    if intents.get("solo"):
        enriched = solo_gate_filter(enriched)

    return enriched


def recommend_from_candidates(
    client: OpenAI,
    payload: Dict[str, Any],
    candidates_for_llm: List[Dict[str, Any]],
) -> Dict[str, Any]:
    intents = infer_intents(payload)
    must_notes = []
    if intents.get("solo"):
        must_notes.append("- ì‚¬ìš©ìê°€ í˜¼ë°¥/1ì¸ì‹ì‚¬ë¥¼ ì›í•œë‹¤. ë‹¨ì²´/íšŒì‹/ëª¨ì„ ì¤‘ì‹¬ ì¥ì†ŒëŠ” ìš°ì„  ë°°ì œí•˜ë¼.")
        must_notes.append("- ë°”ìë¦¬/ì¹´ìš´í„°/1ì¸ì‹ì‚¬ íŒíŠ¸ê°€ ìˆê±°ë‚˜ í˜¼ì ë¨¹ê¸° í¸í•œ í˜•íƒœë¥¼ ìš°ì„ í•˜ë¼.")
    if intents.get("budget"):
        must_notes.append("- ì‚¬ìš©ìê°€ ê°€ì„±ë¹„/ì €ë ´ì„ ì›í•œë‹¤. 'ì˜¤ë§ˆì¹´ì„¸/íŒŒì¸ë‹¤ì´ë‹/ê³ ê¸‰/ì½”ìŠ¤' ëŠë‚Œì€ ë°°ì œí•˜ë¼.")
    if intents.get("vegan"):
        must_notes.append("- ì‚¬ìš©ìê°€ ë¹„ê±´/ì±„ì‹ì„ ì›í•œë‹¤. ê³ ê¸° ì¤‘ì‹¬ ì‹ë‹¹ì€ ë°°ì œí•˜ê³  ì±„ì‹ ì„ íƒì§€ê°€ ìˆëŠ” í›„ë³´ë¥¼ ìš°ì„ í•˜ë¼.")
    if intents.get("diet"):
        must_notes.append("- ì‚¬ìš©ìê°€ ë‹¤ì´ì–´íŠ¸ë¥¼ ì›í•œë‹¤. ìƒëŸ¬ë“œ/ë‹¨ë°±ì§ˆ/ì €íƒ„ìˆ˜ ì˜µì…˜ì´ ê°€ëŠ¥í•œ í›„ë³´ë¥¼ ìš°ì„ í•˜ë¼.")

    must_notes_text = "\n".join(must_notes) if must_notes else "- ì‚¬ìš©ì ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” í›„ë³´ë¥¼ ìš°ì„ í•˜ë¼."

    system_rec = (
        "ë„ˆëŠ” ìŒì‹ì /ì¹´í˜ ì¶”ì²œ íë ˆì´í„°ë‹¤.\n"
        "- ë°˜ë“œì‹œ candidates ëª©ë¡ì— ìˆëŠ” ì¥ì†Œë§Œ ì¶”ì²œí•  ìˆ˜ ìˆë‹¤.\n"
        "- candidatesì— ì—†ëŠ” ì¥ì†Œë¥¼ ìƒˆë¡œ ë§Œë“¤ë©´ ì‹¤íŒ¨ë‹¤.\n"
        "- ìˆ«ì(í‰ì /ê°€ê²©/ê±°ë¦¬/ì‹œê°„)ëŠ” ê·¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ì¶œë ¥ì€ JSONë§Œ.\n"
        f"{must_notes_text}\n"
        "ìŠ¤í‚¤ë§ˆ:\n"
        "{\n"
        "  \"recommendations\": [\n"
        "    {\n"
        "      \"rank\": 1,\n"
        "      \"name\": \"...\",\n"
        "      \"reason\": \"...\",\n"
        "      \"tags\": [\"#ê°€ì„±ë¹„\", \"#í˜¼ë°¥\", \"#ì¡°ìš©í•œ\"],\n"
        "      \"address\": \"...\",\n"
        "      \"category\": \"...\",\n"
        "      \"link\": \"...\"\n"
        "    }\n"
        "  ]\n"
        "}\n"
        f"- recommendationsëŠ” ë°˜ë“œì‹œ ì •í™•íˆ {TOP_K}ê°œë¥¼ ì¶œë ¥í•˜ë¼.\n"
        "- rankëŠ” 1ë¶€í„° ì—°ì†ì´ì–´ì•¼ í•œë‹¤.\n"
        "- tagsëŠ” ì§§ê²Œ 2~5ê°œ."
    )

    slim_candidates = []
    for c in candidates_for_llm[:CANDIDATE_LIMIT_FOR_LLM]:
        slim_candidates.append({
            "name": c.get("name", ""),
            "address": c.get("address", ""),
            "category": c.get("category", ""),
            "link": c.get("link", ""),
            "signal": " ".join((c.get("_score_notes") or [])[:3]),
        })

    llm_payload = {
        "visit_type": payload.get("visit_type", "ìƒê´€ì—†ìŒ"),
        "start_location": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance_pref": payload.get("distance_pref", ""),
        "food_type": payload.get("food_type", []),
        "quick_tags": payload.get("quick_tags", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "top_k": TOP_K,
        "candidates": slim_candidates,
    }
    user_rec = json.dumps(llm_payload, ensure_ascii=False)
    return llm_json(client, system_rec, user_rec)


# ===============================
# ë²„íŠ¼ (UX: ì¬ì¶”ì²œ)
# ===============================
btn1, btn2 = st.columns([1, 1])
with btn1:
    run_search = st.button("ğŸ¤– ì¶”ì²œ ë°›ê¸°", use_container_width=True)
with btn2:
    reroll = st.button("ğŸ”„ í›„ë³´ ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ì¶”ì²œ", use_container_width=True)


# ===============================
# ì‹¤í–‰
# ===============================
if run_search or reroll:
    if not situation.strip():
        st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    require_secrets_or_stop()
    client = OpenAI(api_key=openai_api_key)

    payload = {
        "visit_type": visit_type,
        "start_location": start_location.strip(),
        "situation": situation.strip(),
        "people": people,  # 0ì´ë©´ ìƒê´€ì—†ìŒ
        "distance_pref": distance,
        "food_type": food_type,
        "quick_tags": quick_tags,  # âœ… ì´ì œ ë©”ì¸ ì…ë ¥ ì•„ë˜ì—ì„œ ì„ íƒëœ ê°’
        "exclude": exclude_text.strip(),
        "prefer": prefer_text.strip(),
        "blog_sort": blog_sort_param,
    }
    cache_key = build_cache_key(payload)

    # 1) í›„ë³´ ìˆ˜ì§‘(ìºì‹œ)
    if reroll and st.session_state.get("candidates") and st.session_state.get("candidate_cache_key") == cache_key:
        candidates = st.session_state["candidates"]
    else:
        with st.spinner("ì¡°ê±´ì„ ë¶„ì„ ì¤‘..."):
            try:
                queries = generate_queries(client, payload)
            except Exception:
                st.error("ê²€ìƒ‰ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
                st.stop()

        if not queries:
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ì…ë ¥ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ì£¼ë³€ ì‹¤ì œ í›„ë³´(ìŒì‹ì /ì¹´í˜)ë¥¼ ì°¾ëŠ” ì¤‘..."):
            try:
                candidates = collect_candidates(queries)
            except requests.HTTPError as e:
                st.error(f"ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨(HTTP): {e}")
                st.stop()
            except requests.RequestException as e:
                st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                st.stop()

        if not candidates:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            st.stop()

        st.session_state["candidate_cache_key"] = cache_key
        st.session_state["candidates"] = candidates

    # 2) í›„ë³´ ìŠ¤ì½”ì–´ë§ + í˜¼ë°¥ ê²Œì´íŠ¸ í•„í„°
    with st.spinner("í›„ë³´ë¥¼ ì •êµí•˜ê²Œ ì„ ë³„í•˜ëŠ” ì¤‘..."):
        scored_candidates = score_and_prepare_candidates(payload, candidates, blog_sort_param)

    # (ê²€ì¦: í›„ë³´ ë³´ê¸°)
    with st.expander("ğŸ” ì´ë²ˆ ì¶”ì²œì— ì‚¬ìš©ëœ í›„ë³´ ì •ë³´(ê²€ì¦)"):
        st.write(f"- í›„ë³´ ìˆ˜(ì›ë³¸): **{len(candidates)}ê°œ**")
        st.write(f"- í›„ë³´ ìˆ˜(ìŠ¤ì½”ì–´ë§/í•„í„° í›„): **{len(scored_candidates)}ê°œ**")
        sample_df = pd.DataFrame(scored_candidates[:30])
        cols = [c for c in ["name", "category", "address", "_score"] if c in sample_df.columns]
        st.dataframe(sample_df[cols], use_container_width=True, hide_index=True)

        if debug_mode:
            st.caption("ìƒìœ„ í›„ë³´ ì¼ë¶€ì˜ ì ìˆ˜/íŒì • ë©”ëª¨(ë””ë²„ê·¸)")
            for c in scored_candidates[:10]:
                st.write(f"- **{c.get('name')}** ({c.get('_score')}): {c.get('_score_notes')}")

    # 3) LLM ìµœì¢… ì¶”ì²œ(í›„ë³´ ì¤‘ ì„ íƒ)
    with st.spinner("í›„ë³´ ì¤‘ì—ì„œ ìµœì ì˜ ì¥ì†Œë¥¼ ê³ ë¥´ëŠ” ì¤‘..."):
        try:
            pool = scored_candidates[:LLM_RERANK_POOL]
            r_data = recommend_from_candidates(client, payload, pool)
        except Exception:
            st.error("ì¶”ì²œ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    recommendations = r_data.get("recommendations", [])
    if not isinstance(recommendations, list) or len(recommendations) == 0:
        st.error("ì¶”ì²œ ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    recommendations = [r for r in recommendations if isinstance(r, dict)]
    recommendations = sorted(recommendations, key=lambda x: safe_int(x.get("rank", 999)))
    recommendations = recommendations[:TOP_K]
    recommendations = ensure_k_recommendations(recommendations, scored_candidates, TOP_K)

    fixed_summary = f"ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ TOP {TOP_K} ê²°ê³¼ì…ë‹ˆë‹¤."
    st.success(f"âœ… **{fixed_summary}**")
    st.subheader(f"ğŸ† ì¶”ì²œ TOP {TOP_K} (ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜)")

    for r in recommendations:
        name = r.get("name", "ì´ë¦„ ì—†ìŒ")
        address = r.get("address", "") or ""
        category = r.get("category", "") or "ì •ë³´ ì—†ìŒ"
        reason = r.get("reason", "")
        tags = r.get("tags", [])

        # âœ… ì´ë¯¸ì§€ ê²€ìƒ‰: ë§¤ì¥ëª… + ì£¼ì†Œ íŒíŠ¸(ì• 1~2í† í°)
        addr_hint = " ".join(address.split()[:2]).strip()
        img_query = f"{name} {addr_hint}".strip()

        img_items = []
        try:
            img_items = naver_image_search_cached(
                img_query, naver_client_id, naver_client_secret,
                display=IMAGE_PER_PLACE, sort="sim"
            )
        except Exception:
            img_items = []

        with st.container():
            img_col, info_col = st.columns([1, 2])

            with img_col:
                if img_items:
                    thumb = img_items[0].get("thumbnail")
                    if thumb:
                        st.image(thumb, use_container_width=True)
                        src = img_items[0].get("link")
                        if src:
                            st.link_button("ì´ë¯¸ì§€ ì¶œì²˜", src)
                else:
                    st.caption("ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")

            with info_col:
                st.markdown(f"### {r.get('rank', '')}ï¸âƒ£ {name}")
                if tags and isinstance(tags, list):
                    tag_line = " ".join([t if str(t).startswith("#") else f"#{t}" for t in tags[:10]])
                    st.caption(tag_line)
                st.write(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {reason}")
                st.write(f"ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {category}")
                st.write(f"ğŸ“ **ì£¼ì†Œ**: {address or 'ì •ë³´ ì—†ìŒ'}")

                st.link_button("ğŸ—ºï¸ ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë³´ê¸°", naver_map_search_url(name, address))
                if r.get("link"):
                    st.link_button("ğŸ”— ë„¤ì´ë²„/ì˜ˆì•½ ë§í¬", r["link"])

            if show_reviews:
                q = make_review_query(name, address)
                with st.expander("ğŸ–¼ï¸ ë¸”ë¡œê·¸ í›„ê¸° ë³´ê¸°"):
                    st.caption(f"ê²€ìƒ‰ì–´: {q} | ì •ë ¬: {blog_sort}")
                    try:
                        blog_posts = naver_blog_search_cached(
                            q, naver_client_id, naver_client_secret,
                            display=review_display,
                            sort=blog_sort_param,
                        )
                    except Exception:
                        blog_posts = []
                        st.write("í›„ê¸° ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

                    if not blog_posts:
                        st.write("ê´€ë ¨ ë¸”ë¡œê·¸ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    else:
                        for p in blog_posts[:review_display]:
                            st.markdown(f"- [{p['title']}]({p['link']})")
                            if p.get("desc"):
                                st.caption(p["desc"])

            st.divider()

    st.subheader("ğŸ“‹ ì¶”ì²œ ê²°ê³¼ ìš”ì•½í‘œ")
    df = pd.DataFrame(recommendations)
    cols = [c for c in ["rank", "name", "category", "address", "link"] if c in df.columns]
    st.dataframe(df[cols], use_container_width=True, hide_index=True)

else:
    st.info("ğŸ‘† ì¡°ê±´ì„ ì…ë ¥í•˜ê³  **ì¶”ì²œ ë°›ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
