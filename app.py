import json
import re
import time
import html
from typing import List, Dict, Any, Tuple
from urllib.parse import quote_plus

import requests
import streamlit as st
import pandas as pd
from openai import OpenAI


# ===============================
# ê¸°ë³¸ ì„¤ì •
# ===============================
DEFAULT_PEOPLE = 2
DISTANCE_OPTIONS = ["5ë¶„ ì´ë‚´", "10ë¶„ ì´ë‚´", "ìƒê´€ì—†ìŒ"]
DEFAULT_DISTANCE_INDEX = 2  # "ìƒê´€ì—†ìŒ"

FOOD_OPTIONS = [
    "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ê¸°íƒ€",
    "ì¹´í˜", "ë””ì €íŠ¸"
]
DEFAULT_FOOD_TYPES: List[str] = []

TOP_K = 5

# í›„ë³´ í™•ì¥ / ì„±ëŠ¥
MAX_QUERIES = 6
LOCAL_DISPLAY_PER_QUERY = 5
CANDIDATE_LIMIT_FOR_LLM = 35
REQUEST_SLEEP_SEC = 0.08

# ìŠ¤ì½”ì–´ë§/ê²€ì¦ íŠœë‹
SCORE_CANDIDATE_POOL = 60          # ìŠ¤ì½”ì–´ë§ì— ì‚¬ìš©í•  ìµœëŒ€ í›„ë³´ ìˆ˜(ë§ì„ìˆ˜ë¡ ëŠë¦¼)
BLOG_AUGMENT_TOP_M = 18            # ë¸”ë¡œê·¸ ìŠ¤ë‹ˆí«ìœ¼ë¡œ ì¶”ê°€ ì ìˆ˜ ì¤„ í›„ë³´ ìƒìœ„ Mê°œ
BLOG_SCORE_DISPLAY = 3             # ìŠ¤ì½”ì–´ë§ìš© ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìˆ˜(1~5 ê¶Œì¥)


# ===============================
# ìœ í‹¸
# ===============================
def strip_b_tags(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"</?b>", "", text)


def get_secret(key: str) -> str:
    return str(st.secrets.get(key, "")).strip()


def safe_int(x: Any, default: int = 999) -> int:
    try:
        return int(x)
    except Exception:
        return default


def normalize(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def contains_any(text: str, keywords: List[str]) -> bool:
    t = text or ""
    return any(k in t for k in keywords)


def count_any(text: str, keywords: List[str]) -> int:
    t = text or ""
    return sum(1 for k in keywords if k in t)


def naver_local_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 5,
    sort: str = "comment",
    start: int = 1,
) -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API
    https://openapi.naver.com/v1/search/local.json
    """
    url = "https://openapi.naver.com/v1/search/local.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": max(1, start),
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    results: List[Dict[str, str]] = []
    for it in data.get("items", []):
        results.append(
            {
                "name": strip_b_tags(it.get("title", "")),
                "address": it.get("roadAddress") or it.get("address") or "",
                "category": it.get("category", ""),
                "link": it.get("link", ""),
            }
        )
    return results


def dedupe_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    uniq = []
    for c in candidates:
        key = (normalize(c.get("name", "")), normalize(c.get("address", "")))
        if key in seen:
            continue
        seen.add(key)
        uniq.append(c)
    return uniq


def filter_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ëª…ë°±íˆ ë¹„ì‹ìŒë£Œ ì—…ì¢… ì œê±°(ë³‘ì›/í•™ì›/ë¶€ë™ì‚° ë“±)
    """
    bad_keywords = [
        "í•™ì›", "ê³µì¸ì¤‘ê°œ", "ë¶€ë™ì‚°", "ë¯¸ìš©", "ë„¤ì¼", "í”¼ë¶€", "ì„±í˜•",
        "í—¬ìŠ¤", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ì„¸íƒ", "ìˆ˜ë¦¬", "ì •ë¹„", "ë Œíƒˆ",
        "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ì•½êµ­", "ë³‘ì›", "ì˜ì›", "ì¹˜ê³¼", "í•œì˜ì›",
        "ì£¼ìœ ", "ìë™ì°¨", "ì¸í…Œë¦¬ì–´", "ê°€êµ¬", "ë§ˆíŠ¸",
    ]
    out = []
    for c in candidates:
        blob = f"{c.get('name','')} {c.get('category','')}"
        if any(k in blob for k in bad_keywords):
            continue
        out.append(c)
    return out


def extract_json_from_text(text: str) -> dict:
    text = (text or "").strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    text = text.replace("```json", "```").replace("```", "")
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    candidate = text[start : end + 1]
    return json.loads(candidate)


def llm_json(
    client: OpenAI,
    system: str,
    user: str,
    model: str = "gpt-4.1-mini",
    retries: int = 2
) -> dict:
    for attempt in range(retries + 1):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = resp.choices[0].message.content or ""
        try:
            return extract_json_from_text(text)
        except json.JSONDecodeError:
            if attempt == retries:
                raise
            user = user + "\n\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´."
    raise RuntimeError("Unreachable")


def ensure_k_recommendations(
    recommendations: List[Dict[str, Any]],
    candidates: List[Dict[str, str]],
    k: int,
) -> List[Dict[str, Any]]:
    def _key(name: str, address: str) -> tuple:
        return (normalize(name), normalize(address))

    recs = [r for r in recommendations if isinstance(r, dict)]
    recs = sorted(recs, key=lambda x: safe_int(x.get("rank", 999)))

    picked = set()
    cleaned = []
    for r in recs:
        k0 = _key(r.get("name", ""), r.get("address", ""))
        if k0 in picked:
            continue
        picked.add(k0)
        cleaned.append(r)
    recs = cleaned

    if len(recs) < k:
        for c in candidates:
            k0 = _key(c.get("name", ""), c.get("address", ""))
            if k0 in picked:
                continue
            picked.add(k0)
            recs.append({
                "rank": len(recs) + 1,
                "name": c.get("name", ""),
                "reason": "í›„ë³´ ì¤‘ ì¡°ê±´ê³¼ ë¬´ë‚œí•˜ê²Œ ì˜ ë§ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤.",
                "tags": ["ë¬´ë‚œ", "í›„ë³´ê¸°ë°˜"],
                "address": c.get("address", ""),
                "category": c.get("category", ""),
                "link": c.get("link", ""),
            })
            if len(recs) == k:
                break

    recs = recs[:k]
    for i, r in enumerate(recs, start=1):
        r["rank"] = i
    return recs


def make_review_query(name: str, address: str) -> str:
    name = (name or "").strip()
    address = (address or "").strip()
    addr_hint = " ".join(address.split()[:3])
    q = f"{name} {addr_hint} í›„ê¸°".strip()
    return re.sub(r"\s+", " ", q)


@st.cache_data(ttl=3600, show_spinner=False)
def naver_blog_search_cached(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 3,
    sort: str = "sim",
):
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        items.append({
            "title": strip_b_tags(html.unescape(it.get("title", ""))),
            "link": it.get("link", ""),
            "desc": strip_b_tags(html.unescape(it.get("description", ""))),
            "thumbnail": it.get("thumbnail", ""),
        })
    return items


def naver_map_search_url(place_name: str, address: str = "") -> str:
    q = (place_name or "").strip()
    if address:
        q = f"{q} {address.split()[0]}".strip()
    return f"https://map.naver.com/v5/search/{quote_plus(q)}"


def build_cache_key(payload: Dict[str, Any]) -> str:
    compact = {
        "start": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance": payload.get("distance_pref", ""),
        "food": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "visit_type": payload.get("visit_type", ""),
    }
    return json.dumps(compact, ensure_ascii=False, sort_keys=True)


# ===============================
# (1) Intent ì¶”ì¶œ (LLM)
# ===============================
INTENT_LABELS = [
    "í˜¼ë°¥/1ì¸ì‹ì‚¬",
    "ë¹ ë¥¸ ì´ìš©",
    "ëª¨ì„/íšŒì‹",
    "ë°ì´íŠ¸/ë¶„ìœ„ê¸°",
    "ì¹´í˜/ì¹´ê³µ",
    "í•´ì¥",
    "ìˆ ìë¦¬/ì•ˆì£¼",
    "ì¼ë°˜",
]


def infer_intent(client: OpenAI, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìì˜ ìƒí™©ì„ êµ¬ì¡°ì  ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜(ëª¨ë¸ ì¶œë ¥ì€ 'íŒì •'ì— ì‚¬ìš©, ìµœì¢… ê°•ì œëŠ” ë£°/ìŠ¤ì½”ì–´ê°€ ë‹´ë‹¹)
    """
    system = (
        "ë„ˆëŠ” ìŒì‹ì /ì¹´í˜ ì¶”ì²œì„ ìœ„í•œ 'ì˜ë„(intent) íŒì •ê¸°'ë‹¤.\n"
        "ì¶œë ¥ì€ JSONë§Œ.\n"
        "ìŠ¤í‚¤ë§ˆ:\n"
        "{\n"
        "  \"intent\": \"í˜¼ë°¥/1ì¸ì‹ì‚¬|ë¹ ë¥¸ ì´ìš©|ëª¨ì„/íšŒì‹|ë°ì´íŠ¸/ë¶„ìœ„ê¸°|ì¹´í˜/ì¹´ê³µ|í•´ì¥|ìˆ ìë¦¬/ì•ˆì£¼|ì¼ë°˜\",\n"
        "  \"must_include\": [\"...\"] ,\n"
        "  \"must_exclude\": [\"...\"] ,\n"
        "  \"notes\": \"íŒì • ê·¼ê±° í•œ ì¤„\"\n"
        "}\n"
        "- must_include/must_excludeëŠ” ì—…ì¢…/í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì§§ê²Œ.\n"
        "- ëª¨ë¥´ë©´ intentëŠ” 'ì¼ë°˜'ìœ¼ë¡œ.\n"
        "- ìˆ«ì/ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆë¼."
    )

    food = payload.get("food_type") or []
    food_str = ", ".join(food) if food else "(ì„ íƒ ì—†ìŒ)"

    user = (
        f"visit_type(ì‹œê°„ëŒ€/ì¢…ë¥˜): {payload.get('visit_type')}\n"
        f"ì¶œë°œì§€: {payload.get('start_location') or '(ë¯¸ì…ë ¥)'}\n"
        f"ìƒí™©: {payload.get('situation')}\n"
        f"ì¸ì›: {payload.get('people')}\n"
        f"ì´ë™ê±°ë¦¬: {payload.get('distance_pref')}\n"
        f"ì„ í˜¸ ì¢…ë¥˜: {food_str}\n"
        f"ì œì™¸: {payload.get('exclude') or '(ì—†ìŒ)'}\n"
        f"ì„ í˜¸: {payload.get('prefer') or '(ì—†ìŒ)'}\n\n"
        f"ê°€ëŠ¥í•œ intent ë¼ë²¨: {', '.join(INTENT_LABELS)}"
    )

    data = llm_json(client, system, user)
    intent = data.get("intent", "ì¼ë°˜")
    if intent not in INTENT_LABELS:
        intent = "ì¼ë°˜"
    must_in = data.get("must_include", [])
    must_ex = data.get("must_exclude", [])
    if not isinstance(must_in, list):
        must_in = []
    if not isinstance(must_ex, list):
        must_ex = []
    return {
        "intent": intent,
        "must_include": [normalize(str(x)) for x in must_in if normalize(str(x))],
        "must_exclude": [normalize(str(x)) for x in must_ex if normalize(str(x))],
        "notes": normalize(str(data.get("notes", ""))),
    }


# ===============================
# (2) Intentë³„ ë£°/ìŠ¤ì½”ì–´ í•¨ìˆ˜
# ===============================
RULES = {
    "í˜¼ë°¥/1ì¸ì‹ì‚¬": {
        "hard_exclude": ["ë¬´í•œë¦¬í•„", "ë‹¨ì²´", "ì—°íšŒ", "ì›¨ë”©", "ëŒ€ê´€", "ë·”í˜"],
        "penalty": {
            "ê³ ê¸°": -50, "êµ¬ì´": -50, "ì‚¼ê²¹": -50, "ê°ˆë¹„": -50, "ì–‘ê¼¬ì¹˜": -45,
            "ì£¼ì ": -60, "ìˆ ì§‘": -60, "í˜¸í”„": -60, "í¬ì°¨": -60, "ë°”": -45,
        },
        "bonus": {
            "êµ­ë°¥": +25, "ë¼ë©˜": +25, "ìš°ë™": +20, "ë®ë°¥": +20, "ë°±ë°˜": +20,
            "ë¶„ì‹": +18, "ê¹€ë°¥": +18, "ìƒëŸ¬ë“œ": +18, "ë²„ê±°": +15, "ìŒ€êµ­ìˆ˜": +18,
            "ì´ˆë°¥": +12, "ëˆê¹ŒìŠ¤": +12, "ì œìœ¡": +10,
        },
        "blog_bonus_keywords": ["í˜¼ë°¥", "í˜¼ì", "1ì¸", "ì ì‹¬", "ëŸ°ì¹˜", "ì ì‹¬íŠ¹ì„ ", "íšŒì „ìœ¨", "ë¹ ë¥´ê²Œ"],
    },
    "ë¹ ë¥¸ ì´ìš©": {
        "hard_exclude": ["ì½”ìŠ¤", "ì˜¤ë§ˆì¹´ì„¸", "ë·”í˜", "ëŒ€ê´€"],
        "penalty": {"ì›¨ì´íŒ…": -20, "ì£¼ì ": -30, "í¬ì°¨": -30, "ë°”": -20},
        "bonus": {"ë¶„ì‹": +20, "ê¹€ë°¥": +18, "êµ­ë°¥": +18, "ë¼ë©˜": +15, "ë²„ê±°": +15, "ìƒëŸ¬ë“œ": +12},
        "blog_bonus_keywords": ["íšŒì „ìœ¨", "ë¹¨ë¦¬", "ê¸ˆë°©", "ëŒ€ê¸°", "í¬ì¥", "í‚¤ì˜¤ìŠ¤í¬"],
    },
    "ëª¨ì„/íšŒì‹": {
        "hard_exclude": ["1ì¸", "í˜¼ë°¥"],
        "penalty": {"ì¹´ê³µ": -10},
        "bonus": {"ë‹¨ì²´": +18, "ë£¸": +18, "ê³ ê¸°": +12, "êµ¬ì´": +12, "ì „ê³¨": +10, "í•œì •ì‹": +12},
        "blog_bonus_keywords": ["ë£¸", "ë‹¨ì²´", "íšŒì‹", "ëª¨ì„", "ì˜ˆì•½", "ë„“", "ë‹¨ì²´ì„"],
    },
    "ë°ì´íŠ¸/ë¶„ìœ„ê¸°": {
        "hard_exclude": ["ì…€í”„", "í‘¸ë“œì½”íŠ¸"],
        "penalty": {"ë¶„ì‹": -8, "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ": -8},
        "bonus": {"ì™€ì¸": +12, "íŒŒìŠ¤íƒ€": +12, "ë¸ŒëŸ°ì¹˜": +10, "ë””ì €íŠ¸": +10, "ì¹´í˜": +8},
        "blog_bonus_keywords": ["ë¶„ìœ„ê¸°", "ë°ì´íŠ¸", "ê°ì„±", "ì¡°ëª…", "ì¸í…Œë¦¬ì–´", "ì‚¬ì§„", "ë·°"],
    },
    "ì¹´í˜/ì¹´ê³µ": {
        "hard_exclude": ["ì£¼ì ", "í˜¸í”„", "í¬ì°¨", "ê³ ê¸°", "êµ¬ì´"],
        "penalty": {"ì‹ë‹¹": -5},
        "bonus": {"ì¹´í˜": +30, "ë””ì €íŠ¸": +20, "ë² ì´ì»¤ë¦¬": +18, "ì»¤í”¼": +15},
        "blog_bonus_keywords": ["ì¹´ê³µ", "ë…¸íŠ¸ë¶", "ì½˜ì„¼íŠ¸", "ì¡°ìš©", "ì¢Œì„", "ê³µë¶€", "ì™€ì´íŒŒì´", "ë””ì €íŠ¸"],
    },
    "í•´ì¥": {
        "hard_exclude": ["ë””ì €íŠ¸", "ì¼€ì´í¬"],
        "penalty": {"ì¹´í˜": -10},
        "bonus": {"êµ­ë°¥": +25, "í•´ì¥": +20, "ìˆœëŒ€": +15, "ê°ìíƒ•": +18, "ì¹¼êµ­ìˆ˜": +15, "ë¼ë©˜": +10},
        "blog_bonus_keywords": ["í•´ì¥", "êµ­ë¬¼", "ì‹œì›", "ì–¼í°", "ì†í’€ì´"],
    },
    "ìˆ ìë¦¬/ì•ˆì£¼": {
        "hard_exclude": ["í‚¤ì¦ˆ", "í•™ì›"],
        "penalty": {"ìƒëŸ¬ë“œ": -8},
        "bonus": {"ì£¼ì ": +25, "ìˆ ì§‘": +25, "í˜¸í”„": +20, "í¬ì°¨": +20, "ë°”": +15, "ì•ˆì£¼": +12},
        "blog_bonus_keywords": ["ì•ˆì£¼", "ë¶„ìœ„ê¸°", "ìˆ ", "2ì°¨", "ë§¥ì£¼", "í•˜ì´ë³¼", "ì™€ì¸"],
    },
    "ì¼ë°˜": {
        "hard_exclude": [],
        "penalty": {},
        "bonus": {},
        "blog_bonus_keywords": [],
    },
}


def score_candidate(
    c: Dict[str, str],
    intent: str,
    extra_must_exclude: List[str],
) -> Tuple[int, List[str]]:
    """
    í›„ë³´ 1ê°œë¥¼ (ë£° ê¸°ë°˜) ì ìˆ˜í™”. ì ìˆ˜+ê°„ë‹¨í•œ ì‚¬ìœ  ë¡œê·¸ ë°˜í™˜.
    """
    rule = RULES.get(intent, RULES["ì¼ë°˜"])
    name = normalize(c.get("name", ""))
    cat = normalize(c.get("category", ""))
    blob = f"{name} {cat}"

    score = 0
    reasons = []

    # hard exclude
    hard_ex = rule.get("hard_exclude", []) + (extra_must_exclude or [])
    if hard_ex and contains_any(blob, hard_ex):
        return -10_000, ["í•˜ë“œ ì œì™¸ í‚¤ì›Œë“œ ë§¤ì¹­"]

    # penalty/bonus
    for k, v in (rule.get("penalty", {}) or {}).items():
        if k in blob:
            score += int(v)
            reasons.append(f"íŒ¨ë„í‹°:{k}{v}")

    for k, v in (rule.get("bonus", {}) or {}).items():
        if k in blob:
            score += int(v)
            reasons.append(f"ë³´ë„ˆìŠ¤:{k}+{v}")

    # category ê¸°ë°˜ ì•½í•œ ê°€ì : ì¹´í˜/ë””ì €íŠ¸ ì„ íƒí–ˆëŠ”ë° ê´€ë ¨ ì—…ì¢…ì´ë©´ ê°€ì 
    if "ì¹´í˜" in blob:
        score += 6
    if "ë””ì €íŠ¸" in blob or "ë² ì´ì»¤ë¦¬" in blob:
        score += 4

    return score, reasons


def augment_score_with_blog_snippet(
    c: Dict[str, str],
    base_score: int,
    intent: str,
    naver_client_id: str,
    naver_client_secret: str,
    sort_param: str,
) -> Tuple[int, List[str]]:
    """
    ë¸”ë¡œê·¸ ìŠ¤ë‹ˆí«(desc) í‚¤ì›Œë“œ ê¸°ë°˜ ê°€ì 
    - í˜¸ì¶œ ë¹„ìš©ì´ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ ì¼ë¶€ í›„ë³´ì—ë§Œ ì ìš©í•˜ë„ë¡ ë°”ê¹¥ì—ì„œ ì œì–´
    """
    rule = RULES.get(intent, RULES["ì¼ë°˜"])
    kws = rule.get("blog_bonus_keywords", []) or []
    if not kws:
        return base_score, []

    q = make_review_query(c.get("name", ""), c.get("address", ""))
    try:
        posts = naver_blog_search_cached(
            q,
            naver_client_id,
            naver_client_secret,
            display=BLOG_SCORE_DISPLAY,
            sort=sort_param,
        )
    except Exception:
        return base_score, ["ë¸”ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨(ìŠ¤ì½”ì–´ ë°˜ì˜X)"]

    blob = " ".join([normalize(p.get("desc", "")) for p in posts if isinstance(p, dict)])
    hit = count_any(blob, kws)

    # ê°€ì ì€ ë„ˆë¬´ ì„¸ë©´ í¸í–¥ë˜ë¯€ë¡œ ì™„ë§Œí•˜ê²Œ
    bonus = min(30, hit * 6)  # í‚¤ì›Œë“œ 1ê°œë‹¹ +6, ìµœëŒ€ +30
    if bonus > 0:
        return base_score + bonus, [f"ë¸”ë¡œê·¸í‚¤ì›Œë“œë§¤ì¹­ {hit}ê°œ(+{bonus})"]
    return base_score, []


def rank_candidates_with_rules(
    candidates: List[Dict[str, str]],
    intent_pack: Dict[str, Any],
    naver_client_id: str,
    naver_client_secret: str,
    blog_sort_param: str,
) -> List[Dict[str, Any]]:
    """
    (a) ë£° ê¸°ë°˜ 1ì°¨ ìŠ¤ì½”ì–´ë§
    (b) ìƒìœ„ ì¼ë¶€ë§Œ ë¸”ë¡œê·¸ ìŠ¤ë‹ˆí«ìœ¼ë¡œ ì¶”ê°€ ê°€ì 
    (c) ìµœì¢… ì •ë ¬ í›„ ë°˜í™˜(ì›ë³¸ í•„ë“œ + score í¬í•¨)
    """
    intent = intent_pack.get("intent", "ì¼ë°˜")
    extra_ex = intent_pack.get("must_exclude", []) or []

    pool = candidates[:SCORE_CANDIDATE_POOL]

    scored = []
    for c in pool:
        s, logs = score_candidate(c, intent=intent, extra_must_exclude=extra_ex)
        scored.append({**c, "score": s, "_logs": logs})

    scored.sort(key=lambda x: safe_int(x.get("score", -999999), -999999), reverse=True)

    # ë¸”ë¡œê·¸ ìŠ¤ë‹ˆí« ê°€ì (ìƒìœ„ Mê°œë§Œ)
    top_m = scored[:min(BLOG_AUGMENT_TOP_M, len(scored))]
    rest = scored[min(BLOG_AUGMENT_TOP_M, len(scored)):]
    boosted = []

    for item in top_m:
        s0 = safe_int(item.get("score", 0), 0)
        s1, b_logs = augment_score_with_blog_snippet(
            item, s0, intent=intent,
            naver_client_id=naver_client_id,
            naver_client_secret=naver_client_secret,
            sort_param=blog_sort_param,
        )
        item["score"] = s1
        item["_logs"] = (item.get("_logs", []) or []) + b_logs
        boosted.append(item)

    boosted.sort(key=lambda x: safe_int(x.get("score", -999999), -999999), reverse=True)
    final_scored = boosted + rest
    final_scored.sort(key=lambda x: safe_int(x.get("score", -999999), -999999), reverse=True)
    return final_scored


# ===============================
# (3) ìŠ¤ì½”ì–´ ìƒìœ„ í›„ë³´ë§Œ LLMì— ë„˜ê²¨ ì¶”ì²œ
# ===============================
def generate_queries(client: OpenAI, payload: Dict[str, Any]) -> List[str]:
    system_query = (
        "ë„ˆëŠ” ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIì— ë„£ì„ 'ê²€ìƒ‰ì–´(queries)'ë¥¼ ìƒì„±í•˜ëŠ” ë„ìš°ë¯¸ë‹¤.\n"
        "- ì¥ì†Œ ì´ë¦„ì„ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ë°˜ë“œì‹œ ì‚¬ìš©ìì˜ ì¶œë°œì§€(ì§€ì—­/ì—­/ì£¼ì†Œ) ì •ë³´ë¥¼ queriesì— ë°˜ì˜í•˜ë¼.\n"
        "- ì¶”ì²œ ë°›ì„ ì¢…ë¥˜(ì•„ì¹¨/ì ì‹¬/ì €ë…/ì¹´í˜) ì •ë³´ë¥¼ ë°˜ì˜í•˜ë¼.\n"
        "- ê²€ìƒ‰ì— ì˜ ê±¸ë¦¬ëŠ” ì§§ì€ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ.\n"
        "- ì¶œë ¥ì€ JSONë§Œ. ìŠ¤í‚¤ë§ˆ: { \"queries\": [\"...\", \"...\"] }\n"
        "- queriesëŠ” 3~6ê°œ."
    )

    food = payload.get("food_type") or []
    food_str = ", ".join(food) if food else "(ì„ íƒ ì—†ìŒ)"

    visit = payload.get("visit_type", "ìƒê´€ì—†ìŒ")
    visit_hint = {
        "ì•„ì¹¨": "ì•„ì¹¨/ë¸ŒëŸ°ì¹˜",
        "ì ì‹¬": "ì ì‹¬ ë§›ì§‘",
        "ì €ë…": "ì €ë…/ìˆ ìë¦¬",
        "ì¹´í˜/ë””ì €íŠ¸": "ì¹´í˜/ë””ì €íŠ¸",
        "ìƒê´€ì—†ìŒ": "ë§›ì§‘/ì¹´í˜",
    }.get(visit, "ë§›ì§‘/ì¹´í˜")

    user_query = (
        f"ì¶”ì²œ ì¢…ë¥˜: {visit} ({visit_hint})\n"
        f"ì¶œë°œì§€: {payload.get('start_location') or '(ë¯¸ì…ë ¥)'}\n"
        f"ìƒí™©: {payload.get('situation')}\n"
        f"ì¸ì›: {payload.get('people')}\n"
        f"ì´ë™ê±°ë¦¬ ì„ í˜¸: {payload.get('distance_pref')}\n"
        f"ì„ í˜¸ ìŒì‹/ì¹´í˜ ì¢…ë¥˜: {food_str}\n"
        f"ì œì™¸ ì¡°ê±´: {payload.get('exclude') or '(ì—†ìŒ)'}\n"
        f"ì„ í˜¸ ì¡°ê±´: {payload.get('prefer') or '(ì—†ìŒ)'}\n\n"
        "ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ì— ë„£ì„ queries 3~6ê°œë¥¼ ë§Œë“¤ì–´ì¤˜."
    )

    q_data = llm_json(client, system_query, user_query)
    queries = q_data.get("queries", [])
    queries = [q.strip() for q in queries if isinstance(q, str) and q.strip()]

    # ì¶œë°œì§€ ê°•ì œ í¬í•¨(ê°€ëŠ¥í•˜ë©´)
    start = (payload.get("start_location") or "").strip()
    if start:
        patched = []
        for q in queries:
            patched.append(q if start in q else f"{start} {q}".strip())
        queries = patched

    # ì¤‘ë³µ ì œê±°
    uniq = []
    seen = set()
    for q in queries:
        if q in seen:
            continue
        seen.add(q)
        uniq.append(q)
    return uniq[:MAX_QUERIES]


def collect_candidates(
    queries: List[str],
    naver_client_id: str,
    naver_client_secret: str,
) -> List[Dict[str, str]]:
    candidates: List[Dict[str, str]] = []
    for q in queries[:MAX_QUERIES]:
        candidates.extend(
            naver_local_search(
                query=q,
                client_id=naver_client_id,
                client_secret=naver_client_secret,
                display=LOCAL_DISPLAY_PER_QUERY,
                sort="comment",
                start=1,
            )
        )
        time.sleep(REQUEST_SLEEP_SEC)

    candidates = dedupe_candidates(candidates)
    candidates = filter_candidates(candidates)
    return candidates


def recommend_from_candidates(
    client: OpenAI,
    payload: Dict[str, Any],
    intent_pack: Dict[str, Any],
    ranked_candidates: List[Dict[str, Any]],
    top_k: int,
) -> Dict[str, Any]:
    """
    ìŠ¤ì½”ì–´ ìƒìœ„ í›„ë³´ë§Œ LLMì— ë„˜ê²¨ ìµœì¢… ì„ ì •
    """
    intent = intent_pack.get("intent", "ì¼ë°˜")
    must_ex = intent_pack.get("must_exclude", []) or []
    must_in = intent_pack.get("must_include", []) or []

    system_rec = (
        "ë„ˆëŠ” ìŒì‹ì /ì¹´í˜ ì¶”ì²œ íë ˆì´í„°ë‹¤.\n"
        "- ë°˜ë“œì‹œ candidates ëª©ë¡ì— ìˆëŠ” ì¥ì†Œë§Œ ì¶”ì²œí•  ìˆ˜ ìˆë‹¤.\n"
        "- candidatesì— ì—†ëŠ” ì¥ì†Œë¥¼ ìƒˆë¡œ ë§Œë“¤ë©´ ì‹¤íŒ¨ë‹¤.\n"
        "- ì‚¬ìš©ìì˜ intent(ì˜ë„)ì™€ MUST ì¡°ê±´ì„ ìš°ì„ ìœ¼ë¡œ ì§€ì¼œë¼.\n"
        "- MUST_EXCLUDE ì¡°ê±´ì— ê±¸ë¦¬ëŠ” ì¥ì†ŒëŠ” ì¶”ì²œí•˜ì§€ ë§ˆë¼.\n"
        "- ìˆ«ì(í‰ì /ê°€ê²©/ê±°ë¦¬/ì‹œê°„)ëŠ” ê·¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ì¶œë ¥ì€ JSONë§Œ.\n"
        "ìŠ¤í‚¤ë§ˆ:\n"
        "{\n"
        "  \"summary\": \"í•œ ì¤„ ê²°ë¡ (ìˆ«ì/ê°œìˆ˜ ì–¸ê¸‰ ê¸ˆì§€)\",\n"
        "  \"recommendations\": [\n"
        "    {\n"
        "      \"rank\": 1,\n"
        "      \"name\": \"...\",\n"
        "      \"reason\": \"ì‚¬ìš©ì ì˜ë„ì— ë§ëŠ” ì´ìœ (ì§§ê³  ëª…í™•)\",\n"
        "      \"tags\": [\"#í‚¤ì›Œë“œ\", \"#í‚¤ì›Œë“œ\"],\n"
        "      \"address\": \"...\",\n"
        "      \"category\": \"...\",\n"
        "      \"link\": \"...\"\n"
        "    }\n"
        "  ]\n"
        "}\n"
        f"- recommendationsëŠ” ê°€ëŠ¥í•œ í•œ ì •í™•íˆ {top_k}ê°œë¥¼ ì±„ì›Œë¼.\n"
        "- rankëŠ” 1ë¶€í„° ì—°ì†.\n"
        "- summaryì—ëŠ” '3ê³³/5ê³³' ê°™ì€ ê°œìˆ˜ í‘œí˜„ì„ ì“°ì§€ ë§ˆë¼."
    )

    llm_candidates = []
    for c in ranked_candidates[:CANDIDATE_LIMIT_FOR_LLM]:
        llm_candidates.append({
            "name": c.get("name", ""),
            "address": c.get("address", ""),
            "category": c.get("category", ""),
            "link": c.get("link", ""),
            "score_hint": c.get("score", 0),  # ì°¸ê³ ìš©(ëª¨ë¸ì´ ìˆ«ì ê·¼ê±°ë¡œ ì“°ì§€ ì•Šê²Œ)
        })

    llm_payload = {
        "intent": intent,
        "must_include": must_in,
        "must_exclude": must_ex,
        "visit_type": payload.get("visit_type", "ìƒê´€ì—†ìŒ"),
        "start_location": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance_pref": payload.get("distance_pref", ""),
        "food_type": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "top_k": top_k,
        "candidates": llm_candidates,
    }
    user_rec = json.dumps(llm_payload, ensure_ascii=False)
    return llm_json(client, system_rec, user_rec)


# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="LunchMate ğŸ±", layout="wide")

# ìŠ¤í¬ë¡¤ ì ê¹€ ë°©ì§€ CSS
st.markdown(
    """
    <style>
    html, body { overflow: auto !important; height: auto !important; }
    [data-testid="stAppViewContainer"] { overflow: auto !important; }
    [data-testid="stMain"] { overflow: auto !important; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ½ï¸ LunchMate ğŸ½ï¸")
st.caption(f"ì‚¬ìš©ìë‹˜ì˜ ìƒí™©ê³¼ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì‹ì /ì¹´í˜ í›„ë³´ ì¤‘ ìµœì ì˜ {TOP_K}ê³³ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.")

naver_client_id = get_secret("NAVER_CLIENT_ID")
naver_client_secret = get_secret("NAVER_CLIENT_SECRET")
openai_api_key = get_secret("OPENAI_API_KEY")

if "candidate_cache_key" not in st.session_state:
    st.session_state["candidate_cache_key"] = None
if "candidates" not in st.session_state:
    st.session_state["candidates"] = []
if "ranked_candidates" not in st.session_state:
    st.session_state["ranked_candidates"] = []
if "intent_pack" not in st.session_state:
    st.session_state["intent_pack"] = None


def require_secrets_or_stop():
    if not (naver_client_id and naver_client_secret and openai_api_key):
        st.error("ì„œë¹„ìŠ¤ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()


# ===============================
# ì‚¬ì´ë“œë°”
# ===============================
st.sidebar.header("ğŸ•’ ë§¤ì¥ ë°©ë¬¸ ëª©ì ")
visit_type = st.sidebar.selectbox(
    "ì¶”ì²œ ë°›ì„ ì¢…ë¥˜",
    ["ìƒê´€ì—†ìŒ", "ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì¹´í˜/ë””ì €íŠ¸"],
    index=0
)

st.sidebar.header("ğŸ“ ì¶œë°œ ìœ„ì¹˜(ì •í™•ë„ ê°œì„ )")
start_location = st.sidebar.text_input("ì¶œë°œì§€(íšŒì‚¬/ì—­/ì£¼ì†Œ)", placeholder="ì˜ˆ: ì‹ ì´Œì—­, ê°•ë‚¨ì—­, íŒêµì—­")

st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
people = st.sidebar.slider("ì¸ì› ìˆ˜", 1, 10, DEFAULT_PEOPLE)
distance = st.sidebar.selectbox("ì´ë™ ê±°ë¦¬", DISTANCE_OPTIONS, index=DEFAULT_DISTANCE_INDEX)
food_type = st.sidebar.multiselect("ìŒì‹/ì¹´í˜ ì¢…ë¥˜", FOOD_OPTIONS, default=DEFAULT_FOOD_TYPES)

st.sidebar.header("ğŸš« ì œì™¸ / âœ… ì„ í˜¸")
exclude_text = st.sidebar.text_input("ì œì™¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ë§¤ìš´ ìŒì‹, íšŒ, ì›¨ì´íŒ…")
prefer_text = st.sidebar.text_input("ì„ í˜¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: í˜¼ë°¥, ì¡°ìš©í•œ ê³³, ê°€ì„±ë¹„, ë””ì €íŠ¸")

st.sidebar.header("ğŸ–¼ï¸ í›„ê¸°/ì‚¬ì§„ ì„¤ì •")
show_reviews = st.sidebar.checkbox("ë¸”ë¡œê·¸ í›„ê¸° í‘œì‹œ", value=True)
review_display = st.sidebar.slider("ì¥ì†Œë‹¹ ë¸”ë¡œê·¸ í›„ê¸° ê°œìˆ˜", 1, 3, 2)
blog_sort = st.sidebar.radio("í›„ê¸° ì •ë ¬", ["ì—°ê´€ë„(ì¶”ì²œ)", "ìµœì‹ ìˆœ"], index=0)
blog_sort_param = "sim" if blog_sort.startswith("ì—°ê´€ë„") else "date"


# ===============================
# ë©”ì¸ ì…ë ¥
# ===============================
st.subheader("ğŸ“ í¬ë§ ì¡°ê±´ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”")
situation = st.text_area(
    "ìì—°ìŠ¤ëŸ½ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”(ì·¨í–¥, ë°©ë¬¸ ì§€ì—­, ë°©ë¬¸ì ìˆ˜, ìƒí™© ë“±)",
    placeholder="ì˜ˆ: ì ì‹¬ì— í˜¼ë°¥í•˜ê¸° ì¢‹ì€ ê³³ / íšŒì‹í•˜ê¸° ì¢‹ì€ ê³ ê¹ƒì§‘ / ì¹´ê³µ ê°€ëŠ¥í•œ ì¡°ìš©í•œ ì¹´í˜ ë“±",
)

col1, col2, col3, col4 = st.columns(4)
with col1:
    if st.button("âš¡ ë¹¨ë¦¬ ì´ìš©"):
        situation = "ì‹œê°„ì´ ì—†ì–´ì„œ ë¹¨ë¦¬ ì´ìš©í•  ìˆ˜ ìˆëŠ” ê³³ì„ ì°¾ê³  ìˆì–´ìš”"
with col2:
    if st.button("ğŸ‘¥ ëª¨ì„/íšŒì‹"):
        situation = "ì—¬ëŸ¿ì´ ì¡°ìš©íˆ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ëª¨ì„ ì¥ì†Œê°€ í•„ìš”í•´ìš”"
with col3:
    if st.button("ğŸ¥£ í•´ì¥"):
        situation = "ì–´ì œ ìˆ ì„ ë§ˆì…”ì„œ í•´ì¥ì— ì¢‹ì€ ìŒì‹ì„ ë¨¹ê³  ì‹¶ì–´ìš”"
with col4:
    if st.button("â˜• ì¹´í˜"):
        situation = "ì¹´ê³µí•˜ê¸° ì¢‹ê³  ì½˜ì„¼íŠ¸/ì¢Œì„ì´ ê´œì°®ì€ ì¹´í˜ë¥¼ ì°¾ê³  ìˆì–´ìš”"

st.write("")

# ë²„íŠ¼(ì¬ì¶”ì²œ)
btn1, btn2 = st.columns([1, 1])
with btn1:
    run_search = st.button("ğŸ¤– ì¶”ì²œ ë°›ê¸°", use_container_width=True)
with btn2:
    reroll = st.button("ğŸ”„ í›„ë³´ ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ì¶”ì²œ", use_container_width=True)


# ===============================
# ì‹¤í–‰
# ===============================
if run_search or reroll:
    if not situation:
        st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    require_secrets_or_stop()
    client = OpenAI(api_key=openai_api_key)

    payload = {
        "visit_type": visit_type,
        "start_location": start_location.strip(),
        "situation": situation.strip(),
        "people": people,
        "distance_pref": distance,
        "food_type": food_type,
        "exclude": exclude_text.strip(),
        "prefer": prefer_text.strip(),
    }
    cache_key = build_cache_key(payload)

    # reroll: í›„ë³´/ìŠ¤ì½”ì–´ ì¬ì‚¬ìš©(ì˜ë„ëŠ” ë‹¤ì‹œ ë½‘ì•„ë„ ë˜ì§€ë§Œ, ê¸°ì¤€ ê³ ì •ì´ ë” ë‚˜ì•„ì„œ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©)
    if reroll and st.session_state.get("candidates") and st.session_state.get("candidate_cache_key") == cache_key:
        candidates = st.session_state["candidates"]
        ranked_candidates = st.session_state.get("ranked_candidates", [])
        intent_pack = st.session_state.get("intent_pack") or {"intent": "ì¼ë°˜", "must_include": [], "must_exclude": [], "notes": ""}
    else:
        with st.spinner("ì˜ë„/ê¸°ì¤€ì„ ì •ë¦¬í•˜ëŠ” ì¤‘..."):
            try:
                intent_pack = infer_intent(client, payload)
            except Exception:
                # ì˜ë„ ì¶”ì¶œì´ ì‹¤íŒ¨í•´ë„ ì„œë¹„ìŠ¤ëŠ” ê³„ì† ë™ì‘í•˜ê²Œ(ë³´ìˆ˜ì ìœ¼ë¡œ 'ì¼ë°˜')
                intent_pack = {"intent": "ì¼ë°˜", "must_include": [], "must_exclude": [], "notes": ""}

        with st.spinner("ì¡°ê±´ì„ ë¶„ì„ ì¤‘..."):
            try:
                queries = generate_queries(client, payload)
            except Exception:
                st.error("ê²€ìƒ‰ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
                st.stop()

        if not queries:
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ì…ë ¥ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ì£¼ë³€ ì‹¤ì œ í›„ë³´(ìŒì‹ì /ì¹´í˜)ë¥¼ ì°¾ëŠ” ì¤‘..."):
            try:
                candidates = collect_candidates(queries, naver_client_id, naver_client_secret)
            except requests.HTTPError as e:
                st.error(f"ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨(HTTP): {e}")
                st.stop()
            except requests.RequestException as e:
                st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                st.stop()

        if not candidates:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("í›„ë³´ë¥¼ 'ëª…í™•í•œ ê¸°ì¤€'ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ì¤‘..."):
            ranked_candidates = rank_candidates_with_rules(
                candidates=candidates,
                intent_pack=intent_pack,
                naver_client_id=naver_client_id,
                naver_client_secret=naver_client_secret,
                blog_sort_param=blog_sort_param,
            )

        st.session_state["candidate_cache_key"] = cache_key
        st.session_state["candidates"] = candidates
        st.session_state["ranked_candidates"] = ranked_candidates
        st.session_state["intent_pack"] = intent_pack

    # ê²€ì¦ìš©(ì›í•˜ë©´ ìˆ¨ê²¨ë„ ë¨)
    with st.expander("ğŸ” ì´ë²ˆ ì¶”ì²œì— ì‚¬ìš©ëœ ê¸°ì¤€/í›„ë³´(ê²€ì¦)"):
        st.write(f"- íŒì • intent: **{intent_pack.get('intent','ì¼ë°˜')}**")
        if intent_pack.get("notes"):
            st.caption(f"íŒì • ë©”ëª¨: {intent_pack.get('notes')}")
        if intent_pack.get("must_exclude"):
            st.write(f"- MUST_EXCLUDE: {', '.join(intent_pack.get('must_exclude'))}")
        st.write(f"- í›„ë³´ ìˆ˜: **{len(candidates)}ê°œ**")
        sample_df = pd.DataFrame(ranked_candidates[:25])
        cols = [c for c in ["score", "name", "category", "address"] if c in sample_df.columns]
        st.dataframe(sample_df[cols], use_container_width=True, hide_index=True)

    with st.spinner("ìŠ¤ì½”ì–´ ìƒìœ„ í›„ë³´ ì¤‘ì—ì„œ ìµœì ì˜ ì¥ì†Œë¥¼ ê³ ë¥´ëŠ” ì¤‘..."):
        try:
            r_data = recommend_from_candidates(client, payload, intent_pack, ranked_candidates, TOP_K)
        except Exception:
            st.error("ì¶”ì²œ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    # âœ… summaryëŠ” ê°œìˆ˜ í˜¼ì„  ë°©ì§€ë¥¼ ìœ„í•´ ê³ ì • ë¬¸êµ¬ ì‚¬ìš©
    fixed_summary = f"ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ TOP {TOP_K} ê²°ê³¼ì…ë‹ˆë‹¤."
    recommendations = r_data.get("recommendations", [])

    if not isinstance(recommendations, list) or len(recommendations) == 0:
        st.error("ì¶”ì²œ ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    recommendations = [r for r in recommendations if isinstance(r, dict)]
    recommendations = sorted(recommendations, key=lambda x: safe_int(x.get("rank", 999)))
    recommendations = recommendations[:TOP_K]
    recommendations = ensure_k_recommendations(recommendations, ranked_candidates, TOP_K)

    st.success(f"âœ… **{fixed_summary}**")
    st.subheader(f"ğŸ† ì¶”ì²œ TOP {TOP_K} (ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜)")

    for r in recommendations:
        name = r.get("name", "ì´ë¦„ ì—†ìŒ")
        address = r.get("address", "") or "ì •ë³´ ì—†ìŒ"
        category = r.get("category", "") or "ì •ë³´ ì—†ìŒ"
        reason = r.get("reason", "")
        tags = r.get("tags", [])

        with st.container():
            left, right = st.columns([3, 2])

            with left:
                st.markdown(f"### {r.get('rank', '')}ï¸âƒ£ {name}")
                if tags and isinstance(tags, list):
                    tag_line = " ".join([t if str(t).startswith("#") else f"#{t}" for t in tags[:10]])
                    st.caption(tag_line)
                st.write(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {reason}")
                st.write(f"ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {category}")
                st.write(f"ğŸ“ **ì£¼ì†Œ**: {address}")

            with right:
                st.link_button("ğŸ—ºï¸ ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë³´ê¸°", naver_map_search_url(name, address))
                if r.get("link"):
                    st.link_button("ğŸ”— ë„¤ì´ë²„/ì˜ˆì•½ ë§í¬", r["link"])

            if show_reviews:
                q = make_review_query(name, r.get("address", ""))
                with st.expander("ğŸ–¼ï¸ ë¸”ë¡œê·¸ í›„ê¸° ë³´ê¸°"):
                    st.caption(f"ê²€ìƒ‰ì–´: {q} | ì •ë ¬: {blog_sort}")
                    try:
                        blog_posts = naver_blog_search_cached(
                            q, naver_client_id, naver_client_secret,
                            display=review_display,
                            sort=blog_sort_param,
                        )
                    except Exception:
                        blog_posts = []
                        st.write("í›„ê¸° ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

                    if not blog_posts:
                        st.write("ê´€ë ¨ ë¸”ë¡œê·¸ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    else:
                        for p in blog_posts[:review_display]:
                            cols = st.columns([1, 3])
                            with cols[0]:
                                if p.get("thumbnail"):
                                    st.image(p["thumbnail"], use_container_width=True)
                            with cols[1]:
                                st.markdown(f"- [{p['title']}]({p['link']})")
                                if p.get("desc"):
                                    st.caption(p["desc"])

            st.divider()

    st.subheader("ğŸ“‹ ì¶”ì²œ ê²°ê³¼ ìš”ì•½í‘œ")
    df = pd.DataFrame(recommendations)
    cols = [c for c in ["rank", "name", "category", "address", "link"] if c in df.columns]
    st.dataframe(df[cols], use_container_width=True, hide_index=True)

else:
    st.info("ğŸ‘† ì¡°ê±´ì„ ì…ë ¥í•˜ê³  **ì¶”ì²œ ë°›ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
