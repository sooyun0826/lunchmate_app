import json
import re
import time
import html
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import quote_plus

import requests
import streamlit as st
import pandas as pd
from openai import OpenAI


# ===============================
# ê¸°ë³¸ ë””í´íŠ¸ ì„¤ì • (ì´ˆê¸°ê°’)
# ===============================
PEOPLE_OPTIONS = ["ìƒê´€ì—†ìŒ"] + [f"{i}ëª…" for i in range(1, 11)]
DEFAULT_PEOPLE_INDEX = 0

DISTANCE_OPTIONS = ["5ë¶„ ì´ë‚´", "10ë¶„ ì´ë‚´", "ìƒê´€ì—†ìŒ"]
DEFAULT_DISTANCE_INDEX = 2  # "ìƒê´€ì—†ìŒ"

FOOD_OPTIONS = [
    "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ê¸°íƒ€",
    "ì¹´í˜", "ë””ì €íŠ¸"
]
DEFAULT_FOOD_TYPES: List[str] = []

TOP_K = 5

MAX_QUERIES = 6
LOCAL_DISPLAY_PER_QUERY = 5
CANDIDATE_LIMIT_FOR_LLM = 40
REQUEST_SLEEP_SEC = 0.08

BLOG_PER_PLACE_FOR_SCORING = 3
LLM_RERANK_POOL = 25


# ===============================
# ìœ í‹¸
# ===============================
def strip_b_tags(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"</?b>", "", text)


def get_secret(key: str) -> str:
    return str(st.secrets.get(key, "")).strip()


def safe_int(x: Any, default: int = 999) -> int:
    try:
        return int(x)
    except Exception:
        return default


def normalize_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def split_csv(text: str) -> List[str]:
    parts = [p.strip() for p in (text or "").split(",")]
    return [p for p in parts if p]


def any_kw(blob: str, kws: List[str]) -> bool:
    b = normalize_text(blob)
    return any(normalize_text(k) in b for k in kws if k)


def count_kw_hits(blob: str, kws: List[str]) -> int:
    b = normalize_text(blob)
    hits = 0
    for k in kws:
        kk = normalize_text(k)
        if kk and kk in b:
            hits += 1
    return hits


def parse_people_value(choice: str) -> int:
    choice = (choice or "").strip()
    if choice == "ìƒê´€ì—†ìŒ":
        return 0
    m = re.search(r"(\d+)", choice)
    return int(m.group(1)) if m else 0


def force_https(url: str) -> str:
    u = (url or "").strip()
    if u.startswith("http://"):
        return "https://" + u[len("http://") :]
    return u


def naver_local_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 5,
    sort: str = "comment",
    start: int = 1,
) -> List[Dict[str, str]]:
    url = "https://openapi.naver.com/v1/search/local.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": max(1, start),
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    results: List[Dict[str, str]] = []
    for it in data.get("items", []):
        results.append(
            {
                "name": strip_b_tags(it.get("title", "")),
                "address": it.get("roadAddress") or it.get("address") or "",
                "category": it.get("category", ""),
                "link": it.get("link", ""),
            }
        )
    return results


def dedupe_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    uniq = []
    for c in candidates:
        key = ((c.get("name", "") or "").strip(), (c.get("address", "") or "").strip())
        if key in seen:
            continue
        seen.add(key)
        uniq.append(c)
    return uniq


def filter_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    bad_keywords = [
        "í•™ì›", "ê³µì¸ì¤‘ê°œ", "ë¶€ë™ì‚°", "ë¯¸ìš©", "ë„¤ì¼", "í”¼ë¶€", "ì„±í˜•",
        "í—¬ìŠ¤", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ì„¸íƒ", "ìˆ˜ë¦¬", "ì •ë¹„", "ë Œíƒˆ",
        "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ì•½êµ­", "ë³‘ì›", "ì˜ì›", "ì¹˜ê³¼", "í•œì˜ì›",
        "ì£¼ìœ ", "ìë™ì°¨", "ì¸í…Œë¦¬ì–´", "ê°€êµ¬", "ë§ˆíŠ¸",
    ]
    out = []
    for c in candidates:
        name = (c.get("name") or "").strip()
        cat = (c.get("category") or "").strip()
        blob = f"{name} {cat}"
        if any(k in blob for k in bad_keywords):
            continue
        out.append(c)
    return out


def extract_json_from_text(text: str) -> dict:
    text = (text or "").strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    text = text.replace("```json", "```").replace("```", "")
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    candidate = text[start: end + 1]
    return json.loads(candidate)


def llm_json(
    client: OpenAI,
    system: str,
    user: str,
    model: str = "gpt-4.1-mini",
    retries: int = 2
) -> dict:
    for attempt in range(retries + 1):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = resp.choices[0].message.content or ""
        try:
            return extract_json_from_text(text)
        except json.JSONDecodeError:
            if attempt == retries:
                raise
            user = user + "\n\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´."
    raise RuntimeError("Unreachable")


def ensure_k_recommendations(
    recommendations: List[Dict[str, Any]],
    candidates: List[Dict[str, Any]],
    k: int,
) -> List[Dict[str, Any]]:
    def _key(name: str, address: str) -> tuple:
        return (str(name or "").strip(), str(address or "").strip())

    recs = [r for r in recommendations if isinstance(r, dict)]
    recs = sorted(recs, key=lambda x: safe_int(x.get("rank", 999)))

    picked = set()
    cleaned = []
    for r in recs:
        k0 = _key(r.get("name", ""), r.get("address", ""))
        if k0 in picked:
            continue
        picked.add(k0)
        cleaned.append(r)
    recs = cleaned

    if len(recs) < k:
        for c in candidates:
            k0 = _key(c.get("name", ""), c.get("address", ""))
            if k0 in picked:
                continue
            picked.add(k0)
            recs.append({
                "rank": len(recs) + 1,
                "name": c.get("name", ""),
                "reason": "í›„ë³´ ì¤‘ ì¡°ê±´ê³¼ ë¬´ë‚œí•˜ê²Œ ì˜ ë§ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤.",
                "tags": ["í›„ë³´ê¸°ë°˜"],
                "address": c.get("address", ""),
                "category": c.get("category", ""),
                "link": c.get("link", ""),
            })
            if len(recs) == k:
                break

    recs = recs[:k]
    for i, r in enumerate(recs, start=1):
        r["rank"] = i
    return recs


def make_review_query(name: str, address: str) -> str:
    name = (name or "").strip()
    address = (address or "").strip()
    addr_hint = " ".join(address.split()[:3])
    q = f"{name} {addr_hint} í›„ê¸°".strip()
    return re.sub(r"\s+", " ", q)


@st.cache_data(ttl=3600, show_spinner=False)
def fetch_og_image(url: str) -> str:
    """
    ë¸”ë¡œê·¸ ë§í¬ì—ì„œ ëŒ€í‘œì´ë¯¸ì§€(og:image) ì¶”ì¶œ fallback.
    - ì¼ë¶€ ë¸”ë¡œê·¸ëŠ” í¬ë¡¤ë§ ì°¨ë‹¨/ë™ì  ë¡œë”©ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ -> ì‹¤íŒ¨ ì‹œ "" ë°˜í™˜
    """
    u = (url or "").strip()
    if not u:
        return ""

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        )
    }
    try:
        r = requests.get(u, headers=headers, timeout=6, allow_redirects=True)
        if r.status_code >= 400:
            return ""
        html_text = r.text

        # og:image / twitter:image ìš°ì„ ìˆœìœ„ë¡œ ì¶”ì¶œ
        patterns = [
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
            r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\']([^"\']+)["\']',
        ]
        for pat in patterns:
            m = re.search(pat, html_text, flags=re.IGNORECASE)
            if m:
                return force_https(m.group(1).strip())
        return ""
    except Exception:
        return ""


@st.cache_data(ttl=3600, show_spinner=False)
def naver_blog_search_cached(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 3,
    sort: str = "sim",
):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API
    - thumbnailì´ ë¹ˆ ê°’ì¸ ê²½ìš°ê°€ ë§ì•„ì„œ, og:imageë¡œ fallback ì²˜ë¦¬
    """
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        title = strip_b_tags(html.unescape(it.get("title", "")))
        link = it.get("link", "")
        desc = strip_b_tags(html.unescape(it.get("description", "")))

        thumb = force_https(it.get("thumbnail", "") or "")
        if not thumb:
            # âœ… thumbnail ì—†ìœ¼ë©´ og:image fallback
            thumb = fetch_og_image(link)

        items.append({
            "title": title,
            "link": link,
            "desc": desc,
            "thumbnail": thumb,
        })
    return items


def naver_map_search_url(place_name: str, address: str = "") -> str:
    q = (place_name or "").strip()
    if address:
        q = f"{q} {address.split()[0]}".strip()
    return f"https://map.naver.com/v5/search/{quote_plus(q)}"


def build_cache_key(payload: Dict[str, Any]) -> str:
    compact = {
        "start": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance": payload.get("distance_pref", ""),
        "food": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "visit_type": payload.get("visit_type", ""),
        "blog_sort": payload.get("blog_sort", "sim"),
        "quick_tags": payload.get("quick_tags", []),
    }
    return json.dumps(compact, ensure_ascii=False, sort_keys=True)


# ===============================
# â€œëª…í™•í•œ ê¸°ì¤€â€ ë£°ì…‹ (í˜¼ë°¥ ê°•í™”)
# ===============================
SOLO_HARD_EXCLUDE = ["ì›¨ë”©", "ëŒ€ê´€", "ì—°íšŒ", "í–‰ì‚¬", "ì„¸ë¯¸ë‚˜", "ë·”í˜", "ëŒì”ì¹˜"]
SOLO_STRONG_PENALTY = [
    "ë‹¨ì²´", "ë‹¨ì²´ì„", "ë‹¨ì²´ê°€ëŠ¥", "íšŒì‹", "ëª¨ì„", "ë£¸", "ë£¸ì™„ë¹„", "ê°€ì¡±ëª¨ì„",
    "ì˜ˆì•½í•„ìˆ˜", "ëŒ€í˜•", "ì—°ë§",
]
SOLO_POSITIVE = ["í˜¼ë°¥", "í˜¼ì", "1ì¸", "1ì¸ì‹ì‚¬", "ë°”ìë¦¬", "ì¹´ìš´í„°", "í‚¤ì˜¤ìŠ¤í¬", "í¬ì¥", "í…Œì´í¬ì•„ì›ƒ"]
SOLO_CATEGORY_PENALTY = ["ê³ ê¸°", "ì‚¼ê²¹", "ê°ˆë¹„", "í•œìš°", "ë¬´í•œë¦¬í•„", "ë°”ë² í", "ê³±ì°½", "ë§‰ì°½", "ì°¸ì¹˜", "íšŸì§‘", "ëŒ€ê²Œ", "ì½”ìŠ¤", "ë·”í˜"]

BUDGET_POSITIVE = ["ê°€ì„±ë¹„", "ì €ë ´", "ì°©í•œê°€ê²©", "ë§Œì›", "ë§Œì›ëŒ€", "ì ì‹¬íŠ¹ì„ ", "ì„¸íŠ¸", "ë°±ë°˜"]
BUDGET_NEGATIVE = ["ì˜¤ë§ˆì¹´ì„¸", "íŒŒì¸ë‹¤ì´ë‹", "ì½”ìŠ¤", "í”„ë¦¬ë¯¸ì—„", "ê³ ê¸‰", "ë¹„ì‹¼", "ê³ ê°€"]


def infer_intents(payload: Dict[str, Any]) -> Dict[str, bool]:
    situation = payload.get("situation", "")
    prefer = payload.get("prefer", "")
    people = payload.get("people", 0)

    blob = f"{situation} {prefer}".strip()
    solo = (people == 1) or any_kw(blob, ["í˜¼ë°¥", "í˜¼ì", "1ì¸", "1ì¸ì‹ì‚¬", "í˜¼ìˆ "])
    budget = any_kw(blob, ["ê°€ì„±ë¹„", "ì €ë ´", "ì‹¸ê²Œ", "ì°©í•œê°€ê²©", "ë§Œì›", "ë§Œì›ëŒ€"])
    vegan = any_kw(blob, ["ë¹„ê±´", "vegan", "ì±„ì‹", "ë½í† ", "ì˜¤ë³´"])
    diet = any_kw(blob, ["ë‹¤ì´ì–´íŠ¸", "ì €íƒ„", "í‚¤í† ", "ìƒëŸ¬ë“œ", "ë‹¨ë°±ì§ˆ"])
    return {"solo": solo, "budget": budget, "vegan": vegan, "diet": diet}


def candidate_signal_blob(candidate: Dict[str, str], blog_snippets: List[str]) -> str:
    name = candidate.get("name", "") or ""
    category = candidate.get("category", "") or ""
    addr = candidate.get("address", "") or ""
    blog = " ".join(blog_snippets[:10])
    return f"{name} {category} {addr} {blog}".strip()


def score_candidate_for_payload(
    payload: Dict[str, Any],
    candidate: Dict[str, str],
    blog_snippets: List[str],
    intents: Dict[str, bool],
) -> Tuple[int, Dict[str, Any]]:
    score = 0
    reasons = []

    blob = candidate_signal_blob(candidate, blog_snippets)
    name_cat = f"{candidate.get('name','')} {candidate.get('category','')}".strip()

    exclude_terms = split_csv(payload.get("exclude", ""))
    if exclude_terms and any_kw(blob, exclude_terms):
        score -= 120
        reasons.append(f"ì œì™¸ì¡°ê±´ ë§¤ì¹­(-120): {', '.join(exclude_terms)}")

    if intents.get("solo"):
        if any_kw(blob, SOLO_HARD_EXCLUDE):
            score -= 999
            reasons.append("í˜¼ë°¥: í•˜ë“œ ì œì™¸ ìš©ë„/ì—…ì¢…(-999)")
        hits = count_kw_hits(blob, SOLO_STRONG_PENALTY)
        if hits:
            penalty = min(80 * hits, 240)
            score -= penalty
            reasons.append(f"í˜¼ë°¥: ë‹¨ì²´/ëª¨ì„ ì‹œê·¸ë„({hits}) ê°ì (-{penalty})")

        pos_hits = count_kw_hits(blob, SOLO_POSITIVE)
        if pos_hits:
            bonus = min(50 * pos_hits, 150)
            score += bonus
            reasons.append(f"í˜¼ë°¥: 1ì¸ ì¹œí™” ì‹œê·¸ë„({pos_hits}) ê°€ì (+{bonus})")

        cat_hits = count_kw_hits(name_cat, SOLO_CATEGORY_PENALTY)
        if cat_hits:
            penalty = min(70 * cat_hits, 210)
            score -= penalty
            reasons.append(f"í˜¼ë°¥: ì—…ì¢… íŒ¨ë„í‹°({cat_hits})(-{penalty})")

    if intents.get("budget"):
        pos = count_kw_hits(blob, BUDGET_POSITIVE)
        if pos:
            bonus = min(35 * pos, 140)
            score += bonus
            reasons.append(f"ê°€ì„±ë¹„: ê¸ì • ì‹œê·¸ë„({pos})(+{bonus})")
        neg = count_kw_hits(blob, BUDGET_NEGATIVE)
        if neg:
            penalty = min(60 * neg, 180)
            score -= penalty
            reasons.append(f"ê°€ì„±ë¹„: ê³ ê°€ ì‹œê·¸ë„({neg})(-{penalty})")

    quick_tags = payload.get("quick_tags", []) or []
    if quick_tags:
        if any_kw(blob, quick_tags):
            score += 40
            reasons.append("ë¹ ë¥¸ íƒœê·¸ ë§¤ì¹­(+40)")

    food_types = payload.get("food_type") or []
    if food_types:
        if any_kw(candidate.get("category", ""), food_types) or any_kw(candidate.get("name", ""), food_types):
            score += 35
            reasons.append("ì„ íƒí•œ ìŒì‹/ì¹´í˜ ì¢…ë¥˜ ë§¤ì¹­(+35)")

    visit = payload.get("visit_type", "ìƒê´€ì—†ìŒ")
    if visit == "ì¹´í˜/ë””ì €íŠ¸":
        if any_kw(blob, ["ì¹´í˜", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ì»¤í”¼"]):
            score += 40
            reasons.append("ë°©ë¬¸ëª©ì (ì¹´í˜/ë””ì €íŠ¸) ë§¤ì¹­(+40)")
        else:
            score -= 20
            reasons.append("ë°©ë¬¸ëª©ì (ì¹´í˜/ë””ì €íŠ¸) ë¶ˆì¼ì¹˜(-20)")
    elif visit in ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…"]:
        if any_kw(blob, ["ë¸ŒëŸ°ì¹˜", "ì•„ì¹¨"]):
            score += 15 if visit == "ì•„ì¹¨" else 0
        if any_kw(blob, ["ì ì‹¬íŠ¹ì„ ", "ëŸ°ì¹˜"]):
            score += 15 if visit == "ì ì‹¬" else 0
        if any_kw(blob, ["ìˆ ", "ì•ˆì£¼", "í˜¸í”„", "í¬ì°¨"]):
            score += 15 if visit == "ì €ë…" else 0

    score += 10
    meta = {"score": score, "score_notes": reasons[:8]}
    return score, meta


def solo_gate_filter(sorted_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    kept = []
    dropped = []
    penalty_set = [normalize_text(x) for x in SOLO_STRONG_PENALTY]
    for c in sorted_candidates:
        blob = normalize_text(c.get("_signal_blob", ""))
        if any(k in blob for k in penalty_set):
            dropped.append(c)
        else:
            kept.append(c)

    if len(kept) >= 15:
        return kept

    dropped = sorted(dropped, key=lambda x: safe_int(x.get("_score", -999999)), reverse=True)
    return kept + dropped[: max(0, 15 - len(kept))]


# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="LunchMate ğŸ±", layout="wide")

st.markdown(
    """
    <style>
    html, body { overflow: auto !important; height: auto !important; }
    [data-testid="stAppViewContainer"] { overflow: auto !important; }
    [data-testid="stMain"] { overflow: auto !important; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ½ï¸ LunchMate ğŸ½ï¸")
st.caption(f"ì‚¬ìš©ìë‹˜ì˜ ìƒí™©ê³¼ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì‹ì /ì¹´í˜ í›„ë³´ ì¤‘ ìµœì ì˜ {TOP_K}ê³³ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.")

naver_client_id = get_secret("NAVER_CLIENT_ID")
naver_client_secret = get_secret("NAVER_CLIENT_SECRET")
openai_api_key = get_secret("OPENAI_API_KEY")

if "candidate_cache_key" not in st.session_state:
    st.session_state["candidate_cache_key"] = None
if "candidates" not in st.session_state:
    st.session_state["candidates"] = []
if "quick_tags_main" not in st.session_state:
    st.session_state["quick_tags_main"] = []


# ===============================
# ì‚¬ì´ë“œë°”
# ===============================
st.sidebar.header("ğŸ•’ ë§¤ì¥ ë°©ë¬¸ ëª©ì ")
visit_type = st.sidebar.selectbox(
    "ì¶”ì²œ ë°›ì„ ì¢…ë¥˜",
    ["ìƒê´€ì—†ìŒ", "ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì¹´í˜/ë””ì €íŠ¸"],
    index=0
)

st.sidebar.header("ğŸ“ ì¶œë°œ ìœ„ì¹˜(ì •í™•ë„ ê°œì„ )")
start_location = st.sidebar.text_input("ì¶œë°œì§€(íšŒì‚¬/ì—­/ì£¼ì†Œ)", placeholder="ì˜ˆ: ì‹ ì´Œì—­, ê°•ë‚¨ì—­, íŒêµì—­")

st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
people_choice = st.sidebar.selectbox("ì¸ì› ìˆ˜", PEOPLE_OPTIONS, index=DEFAULT_PEOPLE_INDEX)
people = parse_people_value(people_choice)

distance = st.sidebar.selectbox("ì´ë™ ê±°ë¦¬", DISTANCE_OPTIONS, index=DEFAULT_DISTANCE_INDEX)
food_type = st.sidebar.multiselect("ìŒì‹/ì¹´í˜ ì¢…ë¥˜", FOOD_OPTIONS, default=DEFAULT_FOOD_TYPES)

st.sidebar.header("ğŸš« ì œì™¸ / âœ… ì„ í˜¸")
exclude_text = st.sidebar.text_input("ì œì™¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ë§¤ìš´ ìŒì‹, íšŒ, ì›¨ì´íŒ…")
prefer_text = st.sidebar.text_input("ì„ í˜¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ì¡°ìš©í•œ ê³³, ê°€ì„±ë¹„, ë””ì €íŠ¸")

st.sidebar.header("ğŸ–¼ï¸ í›„ê¸° ì„¤ì •")
show_reviews = st.sidebar.checkbox("ë¸”ë¡œê·¸ í›„ê¸° í‘œì‹œ", value=True)
review_display = st.sidebar.slider("ì¥ì†Œë‹¹ ë¸”ë¡œê·¸ í›„ê¸° ê°œìˆ˜", 1, 3, 2)
blog_sort = st.sidebar.radio("í›„ê¸° ì •ë ¬", ["ì—°ê´€ë„(ì¶”ì²œ)", "ìµœì‹ ìˆœ"], index=0)
blog_sort_param = "sim" if blog_sort.startswith("ì—°ê´€ë„") else "date"

st.sidebar.divider()
debug_mode = st.sidebar.checkbox("ğŸ§ª ë””ë²„ê·¸(í›„ë³´ ì ìˆ˜/í•„í„° ë³´ê¸°)", value=False)


# ===============================
# ë©”ì¸ ì…ë ¥
# ===============================
st.subheader("ğŸ“ í¬ë§ ì¡°ê±´ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”")
situation = st.text_area(
    "ììœ ë¡­ê²Œ ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”(ì·¨í–¥, ë°©ë¬¸ ì§€ì—­, ì¸ì› ìˆ˜, ì‹ì‚¬ ìƒí™© ë“±)",
    placeholder="ì˜ˆ: ì‹ ì´Œì—­ì—ì„œ ì¹œêµ¬ì™€ ì ì‹¬ ë¨¹ì„ê±°ì•¼. ê°€ì„±ë¹„ ì¢‹ì€ ì¤‘ì‹ ìŒì‹ì  ì¶”ì²œí•´ì¤˜. / ì ì‹¤ì—ì„œ ì¹´ê³µí•˜ê¸° ì¢‹ì€ ì¹´í˜ ì°¾ì•„ì¤˜.",
)

st.markdown("### ğŸ§© ë¹ ë¥¸ íƒœê·¸(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)")
QUICK_TAGS = [
    "í˜¼ë°¥", "ì¡°ìš©í•œ", "ê°€ì„±ë¹„", "ì›¨ì´íŒ… ì ì€", "ë§¤ìš´ ìŒì‹",
    "ë°ì´íŠ¸", "ë‹¨ì²´ ê°€ëŠ¥", "í¬ì¥/í…Œì´í¬ì•„ì›ƒ",
    "ë‹¤ì´ì–´íŠ¸", "ë¹„ê±´", "ìƒëŸ¬ë“œ", "ë””ì €íŠ¸", "ë¸ŒëŸ°ì¹˜",
    "ì•¼ì‹", "ìˆ /ì•ˆì£¼", "ì¹´ê³µ",
]
quick_tags = st.multiselect(
    "ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”",
    QUICK_TAGS,
    default=st.session_state.get("quick_tags_main", []),
    key="quick_tags_main",
)

if quick_tags:
    st.success(f"âœ… ë¹ ë¥¸ íƒœê·¸ ì ìš©ë¨: {', '.join(quick_tags)}")
else:
    st.caption("ì„ íƒí•œ ë¹ ë¥¸ íƒœê·¸ê°€ ì—†ì–´ìš”. í•„ìš”í•˜ë©´ ìœ„ì—ì„œ ê³¨ë¼ì£¼ì„¸ìš”.")

st.write("")


def require_secrets_or_stop():
    if not (naver_client_id and naver_client_secret and openai_api_key):
        st.error("ì„œë¹„ìŠ¤ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()


# ===============================
# ë²„íŠ¼
# ===============================
btn1, btn2 = st.columns([1, 1])
with btn1:
    run_search = st.button("ğŸ¤– ì¶”ì²œ ë°›ê¸°", use_container_width=True)
with btn2:
    reroll = st.button("ğŸ”„ í›„ë³´ ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ì¶”ì²œ", use_container_width=True)


# ===============================
# ì‹¤í–‰
# ===============================
if run_search or reroll:
    if not situation.strip():
        st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    require_secrets_or_stop()
    client = OpenAI(api_key=openai_api_key)

    payload = {
        "visit_type": visit_type,
        "start_location": start_location.strip(),
        "situation": situation.strip(),
        "people": people,
        "distance_pref": distance,
        "food_type": food_type,
        "quick_tags": quick_tags,
        "exclude": exclude_text.strip(),
        "prefer": prefer_text.strip(),
        "blog_sort": blog_sort_param,
    }
    cache_key = build_cache_key(payload)

    # 1) í›„ë³´ ìˆ˜ì§‘(ìºì‹œ)
    if reroll and st.session_state.get("candidates") and st.session_state.get("candidate_cache_key") == cache_key:
        candidates = st.session_state["candidates"]
    else:
        with st.spinner("ì¡°ê±´ì„ ë¶„ì„ ì¤‘..."):
            try:
                # ê·¸ëŒ€ë¡œ ìœ ì§€(ë„ˆì˜ ê¸°ì¡´ generate_queries ì‚¬ìš©)
                # ì—¬ê¸°ì„œëŠ” ìƒëµ ì—†ì´ ë™ì‘í•´ì•¼ í•˜ë¯€ë¡œ, ì•„ë˜ì— ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ë‘ëŠ” êµ¬ì¡°ê°€ í•„ìš”í•˜ì§€ë§Œ
                # ì‚¬ìš©ìê°€ ì œê³µí•œ ì „ì²´ ì½”ë“œ ë¬¸ë§¥ìƒ ì´ë¯¸ ìœ„ì— ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ì§€ ì•Šê³ ,
                # ì•„ë˜ì—ì„œ ë°”ë¡œ í˜¸ì¶œí•˜ê¸° ìœ„í•´ generate_queries/collect_candidates ë“±ì„ ìœ„ì— ì´ë¯¸ ì •ì˜í•´ë‘” ìƒíƒœ.
                queries = generate_queries(client, payload)
            except Exception:
                st.error("ê²€ìƒ‰ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
                st.stop()

        if not queries:
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ì…ë ¥ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ì£¼ë³€ ì‹¤ì œ í›„ë³´(ìŒì‹ì /ì¹´í˜)ë¥¼ ì°¾ëŠ” ì¤‘..."):
            try:
                candidates = collect_candidates(queries)
            except requests.HTTPError as e:
                st.error(f"ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨(HTTP): {e}")
                st.stop()
            except requests.RequestException as e:
                st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                st.stop()

        if not candidates:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            st.stop()

        st.session_state["candidate_cache_key"] = cache_key
        st.session_state["candidates"] = candidates

    with st.spinner("í›„ë³´ë¥¼ ì •êµí•˜ê²Œ ì„ ë³„í•˜ëŠ” ì¤‘..."):
        scored_candidates = score_and_prepare_candidates(payload, candidates, blog_sort_param)

    with st.expander("ğŸ” ì´ë²ˆ ì¶”ì²œì— ì‚¬ìš©ëœ í›„ë³´ ì •ë³´(ê²€ì¦)"):
        st.write(f"- í›„ë³´ ìˆ˜(ì›ë³¸): **{len(candidates)}ê°œ**")
        st.write(f"- í›„ë³´ ìˆ˜(ìŠ¤ì½”ì–´ë§/í•„í„° í›„): **{len(scored_candidates)}ê°œ**")
        sample_df = pd.DataFrame(scored_candidates[:30])
        cols = [c for c in ["name", "category", "address", "_score"] if c in sample_df.columns]
        st.dataframe(sample_df[cols], use_container_width=True, hide_index=True)

        if debug_mode:
            st.caption("ìƒìœ„ í›„ë³´ ì¼ë¶€ì˜ ì ìˆ˜/íŒì • ë©”ëª¨(ë””ë²„ê·¸)")
            for c in scored_candidates[:10]:
                st.write(f"- **{c.get('name')}** ({c.get('_score')}): {c.get('_score_notes')}")

    with st.spinner("í›„ë³´ ì¤‘ì—ì„œ ìµœì ì˜ ì¥ì†Œë¥¼ ê³ ë¥´ëŠ” ì¤‘..."):
        try:
            pool = scored_candidates[:LLM_RERANK_POOL]
            r_data = recommend_from_candidates(client, payload, pool)
        except Exception:
            st.error("ì¶”ì²œ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    recommendations = r_data.get("recommendations", [])
    if not isinstance(recommendations, list) or len(recommendations) == 0:
        st.error("ì¶”ì²œ ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    recommendations = [r for r in recommendations if isinstance(r, dict)]
    recommendations = sorted(recommendations, key=lambda x: safe_int(x.get("rank", 999)))
    recommendations = recommendations[:TOP_K]
    recommendations = ensure_k_recommendations(recommendations, scored_candidates, TOP_K)

    st.success(f"âœ… **ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ TOP {TOP_K} ê²°ê³¼ì…ë‹ˆë‹¤.**")
    st.subheader(f"ğŸ† ì¶”ì²œ TOP {TOP_K} (ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜)")

    for r in recommendations:
        name = r.get("name", "ì´ë¦„ ì—†ìŒ")
        address = r.get("address", "") or ""
        category = r.get("category", "") or "ì •ë³´ ì—†ìŒ"
        reason = r.get("reason", "")
        tags = r.get("tags", [])

        with st.container():
            st.markdown(f"### {r.get('rank', '')}ï¸âƒ£ {name}")
            if tags and isinstance(tags, list):
                tag_line = " ".join([t if str(t).startswith("#") else f"#{t}" for t in tags[:10]])
                st.caption(tag_line)

            st.write(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {reason}")
            st.write(f"ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {category}")
            st.write(f"ğŸ“ **ì£¼ì†Œ**: {address or 'ì •ë³´ ì—†ìŒ'}")

            st.link_button("ğŸ—ºï¸ ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë³´ê¸°", naver_map_search_url(name, address))
            if r.get("link"):
                st.link_button("ğŸ”— ë„¤ì´ë²„/ì˜ˆì•½ ë§í¬", r["link"])

            if show_reviews:
                q = make_review_query(name, address)
                with st.expander("ğŸ–¼ï¸ ë¸”ë¡œê·¸ í›„ê¸° ë³´ê¸°"):
                    st.caption(f"ê²€ìƒ‰ì–´: {q} | ì •ë ¬: {blog_sort}")

                    try:
                        blog_posts = naver_blog_search_cached(
                            q, naver_client_id, naver_client_secret,
                            display=review_display,
                            sort=blog_sort_param,
                        )
                    except Exception:
                        blog_posts = []
                        st.write("í›„ê¸° ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

                    if not blog_posts:
                        st.write("ê´€ë ¨ ë¸”ë¡œê·¸ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    else:
                        # âœ… ë§í¬ ì˜†ì— ì‘ì€ ì¸ë„¤ì¼ í‘œì‹œ
                        for p in blog_posts[:review_display]:
                            thumb = (p.get("thumbnail") or "").strip()
                            thumb = force_https(thumb)

                            c1, c2 = st.columns([1, 6])
                            with c1:
                                if thumb:
                                    st.image(thumb, width=64)
                                else:
                                    st.caption("ì¸ë„¤ì¼ ì—†ìŒ")
                            with c2:
                                st.markdown(f"- [{p['title']}]({p['link']})")
                                if p.get("desc"):
                                    st.caption(p["desc"])

            st.divider()

    st.subheader("ğŸ“‹ ì¶”ì²œ ê²°ê³¼ ìš”ì•½í‘œ")
    df = pd.DataFrame(recommendations)
    cols = [c for c in ["rank", "name", "category", "address", "link"] if c in df.columns]
    st.dataframe(df[cols], use_container_width=True, hide_index=True)

else:
    st.info("ğŸ‘† ì¡°ê±´ì„ ì…ë ¥í•˜ê³  **ì¶”ì²œ ë°›ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
