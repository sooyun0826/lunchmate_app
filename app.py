import json
import re
import time
import html
from typing import List, Dict, Any
from urllib.parse import quote_plus

import requests
import streamlit as st
import pandas as pd
from openai import OpenAI


# ===============================
# ê¸°ë³¸ ë””í´íŠ¸ ì„¤ì • (ì´ˆê¸°ê°’)
# ===============================
DEFAULT_PEOPLE = 2
DISTANCE_OPTIONS = ["5ë¶„ ì´ë‚´", "10ë¶„ ì´ë‚´", "ìƒê´€ì—†ìŒ"]
DEFAULT_DISTANCE_INDEX = 2  # "ìƒê´€ì—†ìŒ"

# âœ… â€œìŒì‹ì  + ì¹´í˜â€ê¹Œì§€ í¬í•¨í•˜ë„ë¡ ì˜µì…˜ í™•ì¥
FOOD_OPTIONS = [
    "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ê¸°íƒ€",
    "ì¹´í˜", "ë””ì €íŠ¸"
]
DEFAULT_FOOD_TYPES: List[str] = []

# âœ… ì¶”ì²œ ê°œìˆ˜
TOP_K = 5

# í›„ë³´ í™•ì¥ / ì„±ëŠ¥
MAX_QUERIES = 6
LOCAL_DISPLAY_PER_QUERY = 5
CANDIDATE_LIMIT_FOR_LLM = 40
REQUEST_SLEEP_SEC = 0.08


# ===============================
# ìœ í‹¸
# ===============================
def strip_b_tags(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"</?b>", "", text)


def get_secret(key: str) -> str:
    return str(st.secrets.get(key, "")).strip()


def safe_int(x: Any, default: int = 999) -> int:
    try:
        return int(x)
    except Exception:
        return default


def naver_local_search(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 5,
    sort: str = "comment",
    start: int = 1,
) -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIë¡œ ì‹¤ì¡´ ì¥ì†Œ í›„ë³´ ìˆ˜ì§‘
    https://openapi.naver.com/v1/search/local.json
    """
    url = "https://openapi.naver.com/v1/search/local.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": max(1, start),
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    results: List[Dict[str, str]] = []
    for it in data.get("items", []):
        results.append(
            {
                "name": strip_b_tags(it.get("title", "")),
                "address": it.get("roadAddress") or it.get("address") or "",
                "category": it.get("category", ""),
                "link": it.get("link", ""),
            }
        )
    return results


def dedupe_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    uniq = []
    for c in candidates:
        key = (c.get("name", "").strip(), c.get("address", "").strip())
        if key in seen:
            continue
        seen.add(key)
        uniq.append(c)
    return uniq


def filter_candidates(candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    âœ… ìŒì‹ì /ì¹´í˜ ì¶”ì²œ ì„œë¹„ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ, ëª…ë°±íˆ ë¹„ì‹ìŒë£Œ ì—…ì¢…ë§Œ ì œê±°
    (ë³‘ì›/í•™ì›/ë¶€ë™ì‚° ë“±)
    """
    bad_keywords = [
        "í•™ì›", "ê³µì¸ì¤‘ê°œ", "ë¶€ë™ì‚°", "ë¯¸ìš©", "ë„¤ì¼", "í”¼ë¶€", "ì„±í˜•",
        "í—¬ìŠ¤", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ì„¸íƒ", "ìˆ˜ë¦¬", "ì •ë¹„", "ë Œíƒˆ",
        "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ì•½êµ­", "ë³‘ì›", "ì˜ì›", "ì¹˜ê³¼", "í•œì˜ì›",
        "ì£¼ìœ ", "ìë™ì°¨", "ì¸í…Œë¦¬ì–´", "ê°€êµ¬", "ë§ˆíŠ¸",
    ]
    out = []
    for c in candidates:
        name = (c.get("name") or "").strip()
        cat = (c.get("category") or "").strip()
        blob = f"{name} {cat}"
        if any(k in blob for k in bad_keywords):
            continue
        out.append(c)
    return out


def extract_json_from_text(text: str) -> dict:
    text = (text or "").strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    text = text.replace("```json", "```").replace("```", "")

    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    candidate = text[start : end + 1]
    return json.loads(candidate)


def llm_json(
    client: OpenAI,
    system: str,
    user: str,
    model: str = "gpt-4.1-mini",
    retries: int = 2
) -> dict:
    for attempt in range(retries + 1):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = resp.choices[0].message.content or ""
        try:
            return extract_json_from_text(text)
        except json.JSONDecodeError:
            if attempt == retries:
                raise
            user = user + "\n\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´."
    raise RuntimeError("Unreachable")


def ensure_k_recommendations(
    recommendations: List[Dict[str, Any]],
    candidates: List[Dict[str, str]],
    k: int,
) -> List[Dict[str, Any]]:
    """
    ì¶”ì²œ ê²°ê³¼ê°€ kê°œ ë¯¸ë§Œì´ë©´ candidatesì—ì„œ ë¶€ì¡±ë¶„ì„ ì±„ì›Œ kê°œë¡œ ë§ì¶¤
    - ì¤‘ë³µ(ì´ë¦„+ì£¼ì†Œ) ì œê±°
    - rank 1~k ì¬ì •ë ¬
    """
    def _key(name: str, address: str) -> tuple:
        return (str(name or "").strip(), str(address or "").strip())

    recs = [r for r in recommendations if isinstance(r, dict)]
    recs = sorted(recs, key=lambda x: safe_int(x.get("rank", 999)))

    picked = set()
    cleaned = []
    for r in recs:
        k0 = _key(r.get("name", ""), r.get("address", ""))
        if k0 in picked:
            continue
        picked.add(k0)
        cleaned.append(r)
    recs = cleaned

    if len(recs) < k:
        for c in candidates:
            k0 = _key(c.get("name", ""), c.get("address", ""))
            if k0 in picked:
                continue
            picked.add(k0)
            recs.append({
                "rank": len(recs) + 1,
                "name": c.get("name", ""),
                "reason": "í›„ë³´ ì¤‘ ì¡°ê±´ê³¼ ë¬´ë‚œí•˜ê²Œ ì˜ ë§ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤.",
                "tags": ["ë¬´ë‚œ", "í›„ë³´ê¸°ë°˜"],
                "address": c.get("address", ""),
                "category": c.get("category", ""),
                "link": c.get("link", ""),
                "evidence": ["ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ í›„ë³´ì— ì¡´ì¬"],
            })
            if len(recs) == k:
                break

    recs = recs[:k]
    for i, r in enumerate(recs, start=1):
        r["rank"] = i
    return recs


def make_review_query(name: str, address: str) -> str:
    name = (name or "").strip()
    address = (address or "").strip()
    addr_hint = " ".join(address.split()[:3])
    q = f"{name} {addr_hint} í›„ê¸°".strip()
    return re.sub(r"\s+", " ", q)


@st.cache_data(ttl=3600, show_spinner=False)
def naver_blog_search_cached(
    query: str,
    client_id: str,
    client_secret: str,
    display: int = 3,
    sort: str = "sim",
):
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": max(1, min(display, 5)),
        "start": 1,
        "sort": sort,
    }
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    items = []
    for it in data.get("items", []):
        items.append({
            "title": strip_b_tags(html.unescape(it.get("title", ""))),
            "link": it.get("link", ""),
            "desc": strip_b_tags(html.unescape(it.get("description", ""))),
            "thumbnail": it.get("thumbnail", ""),
        })
    return items


def naver_map_search_url(place_name: str, address: str = "") -> str:
    q = (place_name or "").strip()
    if address:
        q = f"{q} {address.split()[0]}".strip()
    return f"https://map.naver.com/v5/search/{quote_plus(q)}"


def build_cache_key(payload: Dict[str, Any]) -> str:
    compact = {
        "start": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance": payload.get("distance_pref", ""),
        "food": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "visit_type": payload.get("visit_type", ""),
    }
    return json.dumps(compact, ensure_ascii=False, sort_keys=True)


# ===============================
# Streamlit UI
# ===============================
st.set_page_config(page_title="PlaceMate ğŸ½ï¸â˜•", layout="wide")

# âœ… ë¬¸êµ¬/ë„¤ì´ë°: ì ì‹¬ í•œì • ì œê±°
st.title("ğŸ½ï¸ PlaceMate â˜•")
st.caption("ì•„ì¹¨/ì ì‹¬/ì €ë… ìƒê´€ì—†ì´ ìŒì‹ì ê³¼ ì¹´í˜ë¥¼ â€˜ì‹¤ì œ ì¡´ì¬í•˜ëŠ”â€™ í›„ë³´ ì¤‘ì—ì„œ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.")

naver_client_id = get_secret("NAVER_CLIENT_ID")
naver_client_secret = get_secret("NAVER_CLIENT_SECRET")
openai_api_key = get_secret("OPENAI_API_KEY")

if "candidate_cache_key" not in st.session_state:
    st.session_state["candidate_cache_key"] = None
if "candidates" not in st.session_state:
    st.session_state["candidates"] = []


# ===============================
# ì‚¬ì´ë“œë°”
# ===============================
st.sidebar.header("ğŸ•’ ë°©ë¬¸ ëª©ì (ì•„ì¹¨/ì ì‹¬/ì €ë…/ì¹´í˜)")
visit_type = st.sidebar.selectbox(
    "ì¶”ì²œ ë°›ì„ ì¢…ë¥˜",
    ["ìƒê´€ì—†ìŒ", "ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì¹´í˜/ë””ì €íŠ¸"],
    index=0
)

st.sidebar.header("ğŸ“ ì¶œë°œ ìœ„ì¹˜(ì •í™•ë„ ê°œì„ )")
start_location = st.sidebar.text_input("ì¶œë°œì§€(íšŒì‚¬/ì—­/ì£¼ì†Œ)", placeholder="ì˜ˆ: ì‹ ì´Œì—­, ê°•ë‚¨ì—­, íŒêµì—­")

st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
people = st.sidebar.slider("ì¸ì› ìˆ˜", 1, 10, DEFAULT_PEOPLE)
distance = st.sidebar.selectbox("ì´ë™ ê±°ë¦¬", DISTANCE_OPTIONS, index=DEFAULT_DISTANCE_INDEX)
food_type = st.sidebar.multiselect("ìŒì‹/ì¹´í˜ ì¢…ë¥˜", FOOD_OPTIONS, default=DEFAULT_FOOD_TYPES)

st.sidebar.header("ğŸš« ì œì™¸ / âœ… ì„ í˜¸")
exclude_text = st.sidebar.text_input("ì œì™¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ë§¤ìš´ ìŒì‹, íšŒ, ì›¨ì´íŒ…")
prefer_text = st.sidebar.text_input("ì„ í˜¸ ì¡°ê±´(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ì¡°ìš©í•œ ê³³, ê°€ì„±ë¹„, ë””ì €íŠ¸")

st.sidebar.header("ğŸ–¼ï¸ í›„ê¸°/ì‚¬ì§„ ì„¤ì •")
show_reviews = st.sidebar.checkbox("ë¸”ë¡œê·¸ í›„ê¸° í‘œì‹œ", value=True)
review_display = st.sidebar.slider("ì¥ì†Œë‹¹ ë¸”ë¡œê·¸ í›„ê¸° ê°œìˆ˜", 1, 3, 2)
blog_sort = st.sidebar.radio("í›„ê¸° ì •ë ¬", ["ì—°ê´€ë„(ì¶”ì²œ)", "ìµœì‹ ìˆœ"], index=0)
blog_sort_param = "sim" if blog_sort.startswith("ì—°ê´€ë„") else "date"


# ===============================
# ë©”ì¸ ì…ë ¥
# ===============================
st.subheader("ğŸ“ í¬ë§ ì¡°ê±´ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”")
situation = st.text_area(
    "ìì—°ìŠ¤ëŸ½ê²Œ ì…ë ¥í•´ ì£¼ì„¸ìš”(ì·¨í–¥, ë°©ë¬¸ ì§€ì—­, ë°©ë¬¸ì ìˆ˜, ìƒí™© ë“±)",
    placeholder="ì˜ˆ: ì˜¤ëŠ˜ ì €ë…ì— ê°•ë‚¨ì—­ ê·¼ì²˜ì—ì„œ ì¡°ìš©í•œ íŒŒìŠ¤íƒ€ì§‘ ì¶”ì²œí•´ì¤˜. / ì˜¤í›„ì— ë””ì €íŠ¸ ì¹´í˜ ê°€ê³  ì‹¶ì–´.",
)

# âœ… ë¹ ë¥¸ ì…ë ¥ ë²„íŠ¼ë„ â€œì ì‹¬â€ ì „ìš© í‘œí˜„ ì œê±°
col1, col2, col3, col4 = st.columns(4)
with col1:
    if st.button("âš¡ ë¹¨ë¦¬ ë¨¹ê¸°"):
        situation = "ì‹œê°„ì´ ì—†ì–´ì„œ ë¹¨ë¦¬ ì´ìš©í•  ìˆ˜ ìˆëŠ” ê³³ì„ ì°¾ê³  ìˆì–´ìš”"
with col2:
    if st.button("ğŸ‘¥ ëª¨ì„/íšŒì‹"):
        situation = "ì—¬ëŸ¿ì´ ì¡°ìš©íˆ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ëª¨ì„ ì¥ì†Œê°€ í•„ìš”í•´ìš”"
with col3:
    if st.button("ğŸ¥£ í•´ì¥"):
        situation = "ì–´ì œ ìˆ ì„ ë§ˆì…”ì„œ í•´ì¥ì— ì¢‹ì€ ìŒì‹ì„ ë¨¹ê³  ì‹¶ì–´ìš”"
with col4:
    if st.button("â˜• ì¹´í˜"):
        situation = "ë””ì €íŠ¸/ì»¤í”¼ê°€ ê´œì°®ê³  ì‚¬ì§„ ì°ê¸° ì¢‹ì€ ì¹´í˜ë¥¼ ì°¾ê³  ìˆì–´ìš”"

st.write("")


def require_secrets_or_stop():
    if not (naver_client_id and naver_client_secret and openai_api_key):
        st.error("ì„œë¹„ìŠ¤ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()


def generate_queries(client: OpenAI, payload: Dict[str, Any]) -> List[str]:
    system_query = (
        "ë„ˆëŠ” ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ APIì— ë„£ì„ 'ê²€ìƒ‰ì–´(queries)'ë¥¼ ìƒì„±í•˜ëŠ” ë„ìš°ë¯¸ë‹¤.\n"
        "- ì¥ì†Œ ì´ë¦„ì„ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ë°˜ë“œì‹œ ì‚¬ìš©ìì˜ ì¶œë°œì§€(ì§€ì—­/ì—­/ì£¼ì†Œ) ì •ë³´ë¥¼ queriesì— ë°˜ì˜í•˜ë¼.\n"
        "- ì¶”ì²œ ë°›ì„ ì¢…ë¥˜(ì•„ì¹¨/ì ì‹¬/ì €ë…/ì¹´í˜) ì •ë³´ë¥¼ ë°˜ì˜í•˜ë¼.\n"
        "- ê²€ìƒ‰ì— ì˜ ê±¸ë¦¬ëŠ” ì§§ì€ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ.\n"
        "- ì¶œë ¥ì€ JSONë§Œ. ìŠ¤í‚¤ë§ˆ: { \"queries\": [\"...\", \"...\"] }\n"
        "- queriesëŠ” 3~6ê°œ."
    )

    food = payload.get("food_type") or []
    food_str = ", ".join(food) if food else "(ì„ íƒ ì—†ìŒ)"

    visit = payload.get("visit_type", "ìƒê´€ì—†ìŒ")
    visit_hint = {
        "ì•„ì¹¨": "ì•„ì¹¨/ë¸ŒëŸ°ì¹˜",
        "ì ì‹¬": "ì ì‹¬ ë§›ì§‘",
        "ì €ë…": "ì €ë…/ìˆ ìë¦¬",
        "ì¹´í˜/ë””ì €íŠ¸": "ì¹´í˜/ë””ì €íŠ¸",
        "ìƒê´€ì—†ìŒ": "ë§›ì§‘/ì¹´í˜",
    }.get(visit, "ë§›ì§‘/ì¹´í˜")

    user_query = (
        f"ì¶”ì²œ ì¢…ë¥˜: {visit} ({visit_hint})\n"
        f"ì¶œë°œì§€: {payload.get('start_location') or '(ë¯¸ì…ë ¥)'}\n"
        f"ìƒí™©: {payload.get('situation')}\n"
        f"ì¸ì›: {payload.get('people')}\n"
        f"ì´ë™ê±°ë¦¬ ì„ í˜¸: {payload.get('distance_pref')}\n"
        f"ì„ í˜¸ ìŒì‹/ì¹´í˜ ì¢…ë¥˜: {food_str}\n"
        f"ì œì™¸ ì¡°ê±´: {payload.get('exclude') or '(ì—†ìŒ)'}\n"
        f"ì„ í˜¸ ì¡°ê±´: {payload.get('prefer') or '(ì—†ìŒ)'}\n\n"
        "ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ì— ë„£ì„ queries 3~6ê°œë¥¼ ë§Œë“¤ì–´ì¤˜."
    )

    q_data = llm_json(client, system_query, user_query)
    queries = q_data.get("queries", [])
    queries = [q.strip() for q in queries if isinstance(q, str) and q.strip()]

    start = (payload.get("start_location") or "").strip()
    if start:
        patched = []
        for q in queries:
            patched.append(q if start in q else f"{start} {q}".strip())
        queries = patched

    uniq = []
    seen = set()
    for q in queries:
        if q in seen:
            continue
        seen.add(q)
        uniq.append(q)
    return uniq[:MAX_QUERIES]


def collect_candidates(queries: List[str]) -> List[Dict[str, str]]:
    candidates: List[Dict[str, str]] = []
    for q in queries[:MAX_QUERIES]:
        candidates.extend(
            naver_local_search(
                query=q,
                client_id=naver_client_id,
                client_secret=naver_client_secret,
                display=LOCAL_DISPLAY_PER_QUERY,
                sort="comment",
                start=1,
            )
        )
        time.sleep(REQUEST_SLEEP_SEC)

    candidates = dedupe_candidates(candidates)
    candidates = filter_candidates(candidates)
    return candidates


def recommend_from_candidates(client: OpenAI, payload: Dict[str, Any], candidates: List[Dict[str, str]]) -> Dict[str, Any]:
    system_rec = (
        "ë„ˆëŠ” ìŒì‹ì /ì¹´í˜ ì¶”ì²œ íë ˆì´í„°ë‹¤.\n"
        "- ë°˜ë“œì‹œ candidates ëª©ë¡ì— ìˆëŠ” ì¥ì†Œë§Œ ì¶”ì²œí•  ìˆ˜ ìˆë‹¤.\n"
        "- candidatesì— ì—†ëŠ” ì¥ì†Œë¥¼ ìƒˆë¡œ ë§Œë“¤ë©´ ì‹¤íŒ¨ë‹¤.\n"
        "- ìˆ«ì(í‰ì /ê°€ê²©/ê±°ë¦¬/ì‹œê°„)ëŠ” ê·¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.\n"
        "- ì¶œë ¥ì€ JSONë§Œ.\n"
        "ìŠ¤í‚¤ë§ˆ:\n"
        "{\n"
        "  \"summary\": \"í•œ ì¤„ ê²°ë¡ \",\n"
        "  \"recommendations\": [\n"
        "    {\n"
        "      \"rank\": 1,\n"
        "      \"name\": \"...\",\n"
        "      \"reason\": \"...\",\n"
        "      \"tags\": [\"#ë¸ŒëŸ°ì¹˜\", \"#ì¡°ìš©í•¨\", \"#ë””ì €íŠ¸\"],\n"
        "      \"evidence\": [\"candidatesì— ì¡´ì¬\", \"ì¹´í…Œê³ ë¦¬: ...\", \"ì£¼ì†Œ: ...\"],\n"
        "      \"address\": \"...\",\n"
        "      \"category\": \"...\",\n"
        "      \"link\": \"...\"\n"
        "    }\n"
        "  ]\n"
        "}\n"
        f"- recommendationsëŠ” 1~{TOP_K}ê°œ, rankëŠ” 1ë¶€í„°."
    )

    llm_payload = {
        "visit_type": payload.get("visit_type", "ìƒê´€ì—†ìŒ"),
        "start_location": payload.get("start_location", ""),
        "situation": payload.get("situation", ""),
        "people": payload.get("people", 0),
        "distance_pref": payload.get("distance_pref", ""),
        "food_type": payload.get("food_type", []),
        "exclude": payload.get("exclude", ""),
        "prefer": payload.get("prefer", ""),
        "candidates": candidates[:CANDIDATE_LIMIT_FOR_LLM],
        "top_k": TOP_K,
    }
    user_rec = json.dumps(llm_payload, ensure_ascii=False)
    return llm_json(client, system_rec, user_rec)


# ===============================
# ë²„íŠ¼ (UX: ì¬ì¶”ì²œ)
# ===============================
btn1, btn2 = st.columns([1, 1])
with btn1:
    run_search = st.button("ğŸ¤– ì¶”ì²œ ë°›ê¸°", use_container_width=True)
with btn2:
    reroll = st.button("ğŸ”„ í›„ë³´ ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ì¶”ì²œ", use_container_width=True)


# ===============================
# ì‹¤í–‰
# ===============================
if run_search or reroll:
    if not situation:
        st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    require_secrets_or_stop()
    client = OpenAI(api_key=openai_api_key)

    payload = {
        "visit_type": visit_type,
        "start_location": start_location.strip(),
        "situation": situation.strip(),
        "people": people,
        "distance_pref": distance,
        "food_type": food_type,
        "exclude": exclude_text.strip(),
        "prefer": prefer_text.strip(),
    }
    cache_key = build_cache_key(payload)

    if reroll and st.session_state.get("candidates") and st.session_state.get("candidate_cache_key") == cache_key:
        candidates = st.session_state["candidates"]
    else:
        with st.spinner("ì¡°ê±´ì„ ë¶„ì„ ì¤‘..."):
            try:
                queries = generate_queries(client, payload)
            except Exception:
                st.error("ê²€ìƒ‰ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
                st.stop()

        if not queries:
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ì…ë ¥ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ì£¼ë³€ ì‹¤ì œ í›„ë³´(ìŒì‹ì /ì¹´í˜)ë¥¼ ì°¾ëŠ” ì¤‘..."):
            try:
                candidates = collect_candidates(queries)
            except requests.HTTPError as e:
                st.error(f"ë„¤ì´ë²„ ì§€ì—­ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨(HTTP): {e}")
                st.stop()
            except requests.RequestException as e:
                st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                st.stop()

        if not candidates:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            st.stop()

        st.session_state["candidate_cache_key"] = cache_key
        st.session_state["candidates"] = candidates

    with st.expander("ğŸ” ì´ë²ˆ ì¶”ì²œì— ì‚¬ìš©ëœ í›„ë³´ ì •ë³´(ê²€ì¦)"):
        st.write(f"- í›„ë³´ ìˆ˜: **{len(candidates)}ê°œ**")
        sample_df = pd.DataFrame(candidates[:20])
        cols = [c for c in ["name", "category", "address"] if c in sample_df.columns]
        st.dataframe(sample_df[cols], use_container_width=True, hide_index=True)

    with st.spinner("í›„ë³´ ì¤‘ì—ì„œ ìµœì ì˜ ì¥ì†Œë¥¼ ê³ ë¥´ëŠ” ì¤‘..."):
        try:
            r_data = recommend_from_candidates(client, payload, candidates)
        except Exception:
            st.error("ì¶”ì²œ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”. (OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)")
            st.stop()

    summary = r_data.get("summary", "ì¶”ì²œ ê²°ê³¼ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    recommendations = r_data.get("recommendations", [])

    if not isinstance(recommendations, list) or len(recommendations) == 0:
        st.error("ì¶”ì²œ ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    recommendations = [r for r in recommendations if isinstance(r, dict)]
    recommendations = sorted(recommendations, key=lambda x: safe_int(x.get("rank", 999)))
    recommendations = recommendations[:TOP_K]
    recommendations = ensure_k_recommendations(recommendations, candidates, TOP_K)

    st.success(f"âœ… **{summary}**")
    st.subheader(f"ğŸ† ì¶”ì²œ TOP {TOP_K} (ë„¤ì´ë²„ í›„ë³´ ê¸°ë°˜)")

    for r in recommendations:
        name = r.get("name", "ì´ë¦„ ì—†ìŒ")
        address = r.get("address", "") or "ì •ë³´ ì—†ìŒ"
        category = r.get("category", "") or "ì •ë³´ ì—†ìŒ"
        reason = r.get("reason", "")
        tags = r.get("tags", [])
        evidence = r.get("evidence", [])

        with st.container():
            left, right = st.columns([3, 2])

            with left:
                st.markdown(f"### {r.get('rank', '')}ï¸âƒ£ {name}")
                if tags and isinstance(tags, list):
                    tag_line = " ".join([t if str(t).startswith("#") else f"#{t}" for t in tags[:10]])
                    st.caption(tag_line)
                st.write(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {reason}")
                st.write(f"ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {category}")
                st.write(f"ğŸ“ **ì£¼ì†Œ**: {address}")

            with right:
                st.link_button("ğŸ—ºï¸ ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë³´ê¸°", naver_map_search_url(name, address))
                if r.get("link"):
                    st.link_button("ğŸ”— ë„¤ì´ë²„/ì˜ˆì•½ ë§í¬", r["link"])

            if evidence and isinstance(evidence, list):
                with st.expander("ğŸ§¾ ì¶”ì²œ ê·¼ê±°(ìš”ì•½)"):
                    for ev in evidence[:8]:
                        if ev:
                            st.write(f"- {ev}")

            if show_reviews:
                q = make_review_query(name, r.get("address", ""))
                with st.expander("ğŸ–¼ï¸ ë¸”ë¡œê·¸ í›„ê¸° ë³´ê¸°"):
                    st.caption(f"ê²€ìƒ‰ì–´: {q} | ì •ë ¬: {blog_sort}")
                    try:
                        blog_posts = naver_blog_search_cached(
                            q, naver_client_id, naver_client_secret,
                            display=review_display,
                            sort=blog_sort_param,
                        )
                    except Exception:
                        blog_posts = []
                        st.write("í›„ê¸° ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

                    if not blog_posts:
                        st.write("ê´€ë ¨ ë¸”ë¡œê·¸ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    else:
                        for p in blog_posts[:review_display]:
                            cols = st.columns([1, 3])
                            with cols[0]:
                                if p.get("thumbnail"):
                                    st.image(p["thumbnail"], use_container_width=True)
                            with cols[1]:
                                st.markdown(f"- [{p['title']}]({p['link']})")
                                if p.get("desc"):
                                    st.caption(p["desc"])

            st.divider()

    st.subheader("ğŸ“‹ ì¶”ì²œ ê²°ê³¼ ìš”ì•½í‘œ")
    df = pd.DataFrame(recommendations)
    cols = [c for c in ["rank", "name", "category", "address", "link"] if c in df.columns]
    st.dataframe(df[cols], use_container_width=True, hide_index=True)

else:
    st.info("ğŸ‘† ì¡°ê±´ì„ ì…ë ¥í•˜ê³  **ì¶”ì²œ ë°›ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
